---
title: "Forecasting historical atmospheric carbon dioxide levels above Mauna Loa, Hawaii using the SARIMA model family."
author: "John Robin Inston"
date: "2023-06-17"
categories: [Atmospheric Modelling, Time Series, Statistics, R, SARIMA]
toc: true
execute: 
  echo: false
  warning: false
  message: false
---

**Abstract:**

In this report we aim to select and fit an appropriate Seasonal Autoregressive Integrated Moving Average (SARIMA) model in order to produce accuruate five year ahead forecasts of atmospheric CO2 levels above Mauna Loa, Hawaii. We investigate the underlying properties of the time series data, the suitability of SARIMA models, and whether the forecasts they produce accurately capture future observations. Throughout the project we implement various univariate time series analysis techniques including exploratory analysis, model selection procedures based on the adjusted Akaike Information Criteria, diagnostic checking, residual analysis and spectral analysis. Our first choice $\text{SARIMA}(0,1,3)(0,1,1)_{12}$ model performed well, producing approximately Gaussian WN residuals and accurate forecasts with confidence intervals capturing the true observed future values of the time series.

```{r}
# load libraries
library(tsdl) # source for data
library(zoo) # rollmean
library(tidyverse)
library(gridExtra) # multiple ggplots
library(forecast) # seasonal plot
library(tseries) # adf test function
library(kableExtra) # tables
library(astsa) # acf2,
library(LSTS) 
#library(MASS) # box-cox (potentially remove)
library(TSA)
library(ggpubr) # qqplot
library(lmtest)
# time series decomposition
devtools::install_github("brisneve/ggplottimeseries")
library(ggplottimeseries)
# set plot information
theme_set(theme_minimal())
# set seed
set.seed(8467)
```

## Introduction

Several geophysical time series contain seasonal variation with one key example being carbon dioxide (CO$_2$) atmospheric concentration which typically is known to have annual seasonality - largely due to plant photosynthesis and respiration - superimposed upon an increasing trend caused primarily by human CO$_2$ production. In this project we examine a data set consisting of monthly measurements of atmospheric CO$_2$ above Mauna Lao, Hawaii in the 32 year period from between 1959 and 1990 with the aim of producing accurate forecasts - in our case a 5 year ahead forecast - by fitting a suitable model from the SARIMA model family. We use a variety of time series analysis including exploratory analysis (time series decomposition, stationarity transformations, inspecting series autocorrelations and partial autocorrelations, tests for normality, stationarity and autocorrelation), SARIMA fitting and model selection (checking for stationarity and invertibility, coefficient significance testing, performance criteria) and residual analysis (portmanteau tests, residual WN tests, model spectral decomposition). We find that the $\text{SARIMA}(0,1,3)(0,1,1)_{12}$ model is our optimal model candidate and our subsequent residual analysis, forecasting and spectral decomposition indicates that our model produces accurate forecasts which captured the future true observations of the time
series. Our analysis was performed using the programming language R and a full breakdown of our code can be found in the corresponding GitHub repository.

## Data

```{r}
# load data
tsdl_met <- subset(tsdl, 12, "Meteorology")
rawdata <- tsdl_met[[2]]
# select training data
data <- head(rawdata, n=length(rawdata)-(12*5))
```

### Data Description

Our data set contains monthly measurements of atmospheric carbon dioxide above Mauna Loa, Hawaii from January 1959 to December 1990, measured in parts per million (ppm). The data was originally collected from the Scripps Institute of Oceanography, La Jolla, California and made readily available through the `tsdlpackage` developed by Rob Hyndman, Professor of Statistics at Monash University, Australia. The data set initially contained missing values which were subsequently filled in by linear interpolation. Our raw data series, labelled by $x_t$, $t = 1, ..., 384$ contains 384 observations.

### Exploratory Analysis

We split our data into disjoint training and testing sets, with the latter containing the last 5 years of observed CO$_2$ levels to be used to analyse the performance of the foreacsts produced by our chosen final model. Our training data set contains 324 observations whilst our testing data set contains 60. Using the training data
we produce a time series plot, shown below in @fig-edatsplot.

```{r}
#| fig-height: 5
#| fig-width: 10
#| fig-cap: "Atmospheric carbon dioxide above Mauna Loa, Hawaii."
#| label: fig-edatsplot
## plot of training data (with trend line)
# trend line
maline <- rollmean(data, k=12, fill=NA, align='right')
# produce time series plot
plotdata <- data.frame(
    "Time" = seq(as.Date("1959/1/1"), as.Date("1985/12/1"), "months"),
    "data" = data,
    "trend" = maline
)
plotdata |>
    tidyr::gather("id", "value", 2:3) |>
    ggplot(aes(Time, value, col=id, linetype=id)) +
    geom_line() +
    theme_light() +
    labs(
        x = "Time", 
        y = "CO2 (ppm)", 
        color="Legend", 
        linetype="Legend"
        ) +
    theme(legend.position = "bottom")
```

We see clear evidence of both an increasing polynomial trend and a seasonal component (with a 12 month period) implying that our raw data is non-stationary. The variance of our data appears constant and there is no evidence of sharp changes in behaviour indicating that a SARIMA model will likely suit our data well. We decompose the time series in @fig-decomposition below to provide a clearer picture of the seasonal and trend components and since we observe no evidence of heteroscedastic behaviour in the residuals we opt not to transform our data.

```{r}
#| fig-height: 10
#| fig-width: 10
#| fig-cap: "Time Series Decomposition"
#| label: fig-decomposition

## time series decomposition
ts_decomp <- ggdecompose(
    dts2(data, type ="additive")) +
    labs(
        x = "Date",
        y = "Atmospheric Concentration of CO2",
        title = "Time Series Decomposition Plots"
    )
# seasonal plot
seasplot <- ggseasonplot(
    data, 
    year.labels=TRUE, 
    year.labels.left=TRUE
    ) +
    labs(
        y = "CO2 (ppm)",
        title = "Seasonal plot"
    )
trendplot <- autoplot(
    maline, 
    ts.color="red", 
    main="Trend plot")
grid.arrange(
    ts_decomp, seasplot, trendplot,
    layout_matrix = rbind(c(1, 1),
    c(2, 3))
    )
```

We produce a plot of the autocorrelation function (ACF) of the data, shown below in @fig-acfundiff, from which we see highly significant autocorrelations for all lags up to 40 which slowly decay and show seasonality.

```{r}
#| fig-cap: ACF and PACF of raw data.
#| label: fig-acfundiff

## acf of undifferenced data
acf2(data, 40, main="ACF and PACF of raw data.") |> invisible()
```

We attempt to obtain stationarity by differencing, first with lag 12 to remove the seasonal trend. Mathematically this transformation is denoted:

\begin{equation}\tag{1}
y_t = x_t−x_{t−12} = (1−B^{12})x_t = ∇_{12}x_t,
\end{equation}

where $y_t$ is our seasonally differenced data series and B is the back-shift operator whereby $B^kx_t = x_{t−k}$. A time series plot of our differenced data is shown below in @fig-acfseasonaldiff along with a trend line, from which we can see that all evidence of a seasonal trend has been removed but the data appears to still be non-stationary as a linear trend is still present. Alongside we include the ACF plot of our seasonally differenced data from which we see the decaying autocorrelation typical of data with a polynomial trend.

```{r}
#| fig-height: 6
#| label: fig-acfseasonaldiff
#| fig-cap: "Time series plot, ACF and PACF of seasonally differenced data."

# differencing data
data_d <- diff(data, 12)
# time series plot of differenced data
plot_data_d <- data.frame(
"Time" = seq(as.Date("1960/1/1"), as.Date("1985/12/1"), "months"),
"yt" = data_d
)
tsplot_data_d <- plot_data_d %>%
tidyr::gather("id", "value", 2:2) %>%
ggplot(., aes(Time, value))+
geom_line() +
theme_minimal() +
labs(x = "Time", y = "Seasonally Differenced Monthly CO2") +
geom_smooth(method=lm) #add linear trend line

# acf plot of differenced data
acfplot_data_d <- autoplot(acf(data_d, 40, plot = FALSE, main = "")) +
    labs(title = "")
pacfplot_data_d <- autoplot(pacf(data_d, 40, plot = FALSE)) +
    labs(title = "")

grid.arrange(
    tsplot_data_d,acfplot_data_d,pacfplot_data_d, 
    layout_matrix = rbind(c(1, 1),
    c(2, 3)))
```

We therefore difference again with lag 1 to remove the linear trend from in the data, denoted mathematically by:

\begin{equation}\tag{2}
u_t = y_t−y_{t−1} = (1−B)y_t = (1−B)(1−B^{12})x_t = ∇_1∇_{12}x_t,
\end{equation}

where $u_t$ is our twice differenced data. We produce another time series plot of our twice differenced data below in @fig-tsdiff.

```{r}
#| fig-width: 10
#| fig-height: 5
#| label: fig-tsdiff

# difference again with lag 1
data_dd <- diff(data_d, 1)
# plot of transformed data
# time series plot of differenced data
plot_data_dd <- data.frame(
"Time" = seq(as.Date("1960/2/1"), as.Date("1985/12/1"), "months"),
"yt" = data_dd
)
plot_data_dd %>%
tidyr::gather("id", "value", 2:2) %>%
ggplot(., aes(Time, value))+
geom_line() +
theme_minimal() +
labs(x = "Time", y = "Transformed Data (zt)")
```

Our twice differenced data now appears stationary with no evidence of seasonality or polynomial trends with a stable variance in time. In @tbl-variance below we see that each transformation lowered the variance of our data.

```{r}
#| label: tbl-variance
#| tbl-cap: "Transformed data variance stationarity test result."

# table of variance of differenced data
data.frame(
"Transformation" = c("None", "Differenced lag 12", "Differenced lag 12 and lag 1"),
"Variance" = c(var(data), var(data_d), var(data_dd))
) |>
    kable()
 
```

In @fig-histdiff we see that the ACF of our twice differenced data shows no evidence of polynomial or seasonal trends. Further both the histogram and Q-Q plot shows that the empirical distribution of our data is symmetric and approximately Gaussian.

```{r}
#| fig-height: 6
#| fig-weight: 10
#| label: fig-histdiff
#| fig-cap: "ACF, PACF, histogram and Q-Q plot of twice differenced data."

# acf of differenced data
acf_data_dd <- autoplot(acf(data_dd, lag=40, plot = FALSE)) + labs(title = "")
pacf_data_dd <- autoplot(pacf(data_dd, lag = 40, plot = FALSE)) + labs(title = "")
# histogram of differenced data
hist_data_dd <- ggplot(data.frame("data"=data_dd), aes(x=data)) +
geom_histogram(color="darkblue", fill="lightblue") +
ggtitle("Histogram")
# q-q plot of differenced data
qqplot_data_dd <- ggqqplot(data_dd, color="darkblue") + ggtitle("Q-Q plot")
grid.arrange(acf_data_dd, pacf_data_dd, hist_data_dd, qqplot_data_dd,ncol=2, nrow=2)
# stationarity tests (adf and kpss)
adf_test <- adf.test(data_dd)
kpss_test <- kpss.test(data_dd, null="Trend")
table_data <- data.frame(
Test = c("ADF Test", "KPSS Test"),
Null = c("Process has a unit root and therefore a trend.",
"Process is stationary"),
pValue = c("<0.01", ">0.1"),
Conclusion = c("Data is stationary", "Data is stationary")
) 
```

Finally, in @tbl-adftest we see the p-values for both the Augmented Dickey–Fuller (ADF) t-statistic test (which
examines whether the series has a unit root where a series with a trend line will have a unit root and result
in a large p-value); and the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test (which tests the null hypothesis
of trend stationarity where a low p-value will indicate a signal that is not trend stationary, has a unit root),
both of which concluded that the twice differenced data is trend stationary.

```{r}
#| label: tbl-adftest
#| tbl-cap: "ADF and KPSS test result."

adf_test <- adf.test(data_dd)
kpss_test <- kpss.test(data_dd, null="Trend")
data.frame(
Test = c("ADF Test", "KPSS Test"),
Null = c("Process has a unit root and therefore a trend.",
"Process is stationary"),
pValue = c("<0.01", ">0.1"),
Conclusion = c("Data is stationary", "Data is stationary")
) |>
    kable()
```


## Model Fitting

### Parameter Selection

When considering a potential SARIMA model we are required to select possible values for the 7 parameters
$p$, $d$, $q$, $P$, $D$, $Q$ and $s$. Since we differenced our data with lag 1 once and with lag 12 once to obtain stationarity we have the parameters $d=1$, $D=1$ and $s = 12$. To determine a potential range of values of the remaining parameters (AR parameters $p$, $P$ and MA parameters $q$, $Q$) we examine both the ACF and PACF plots of the differenced data, shown above in @fig-histdiff.

To determine an upper bound for the AR term $p$ range we look at the PACF and see that we have significant partial-autocorrelation spikes at lags 1 and 3 indicating we may require up to $p= 3$. We also note there are significant spikes at lags 9 and 11 although these are likely interactions with seasonal components to be considered later. To determine an upper bound for the MA term $q$ range we look at the ACF and see that we have significant autocorrelation spikes at lags 1 and 3, indicating we may require up to $q= 3$. Similarly, we also see significant spikes at lags 9 and 11, likely due to interactions with seasonal components.

Now considering seasonal AR terms we again turn to the PACF, this time noting that we have significant partial-autocorrelation at lags 12,24 and 36 indicating that we may require a seasonal AR parameter up to $P= 3$. Finally, to determine an upper bound for the seasonal MA term Q range we note from the ACF plot that we have significant autocorrelation at lag 12 only, indicating the we likely require a seasonal MA parameter of $Q= 1$.

We proceed to select the two models using a procedure whereby we systematically fit, evaluate and compare models using the Akaike information criterion (adjusted for small sample sizes) - denoted AICc. Earlier we determined that at most our model with have up to $p= 3$, $q= 3$, $P= 3$ and $Q= 1$ however, since the AICc punishes models with large numbers of parameters our approach to selecting an optimal model will be to start with the simplest models and add parameters with the aim of minimizing the AICc.

```{r}
#| label: tbl-modelsummary
#| tbl-cap: "Model comparison summary (Significance codes: 0 $\\text{***}$, 0.001 $\\text{**}$, 0.01 $\\text{*}$, 0.05 .)."

data.frame(
    Model = c(
        "SARIMA(0,1,1)(0,1,0)12",
        "SARIMA(0,1,0)(0,1,1)12",
        "SARIMA(0,1,2)(0,1,0)12",
        "SARIMA(0,1,1)(0,1,1)12",
        "SARIMA(0,1,3)(0,1,0)12",
        "SARIMA(0,1,2)(0,1,1)12",
        "SARIMA(0,1,3)(0,1,1)12",
        "SARIMA(0,1,3)(0,1,0)12 (MA2=0)",
        "SARIMA(0,1,3)(0,1,1)12 (MA2=0)",
        ##
        "SARIMA(1,1,1)(0,1,1)12",
        "SARIMA(0,1,1)(1,1,1)12",
        "SARIMA(1,1,3)(0,1,1)12 (MA2=0)",
        "SARIMA(3,1,3)(0,1,1)12 (MA2=0)",
        "SARIMA(3,1,1)(0,1,1)12",
        "SARIMA(3,1,1)(0,1,1)12 (AR1=AR2=0)"
        ),
    ar1 = c(
        " ", " ", " ", " ", " ", " ", " ", " ", " ",
        "0.345*", " ", "0.026", "0.660", "0.371*", "0"
            ),
    ar2 = c(
        " ", " ", " ", " ", " ", " ", " ", " ", " ",
        " ", " ", " ", "0.235", "0.139.", "0"
    ),
    ar3 = c(
        " ", " ", " ", " ", " ", " ", " ", " ", " ",
        " ", " ", " ", "-0.269", "-0.103", "-0.139*"
    ),
    ma1 = c(
        "-0.344***", " ", "-0.346***", "-0.353***",
        "-0.325***", "-0.354***", "-0.356***",
        "-0.322***", "-0.355***", "-0.675***",
        "-0.357***", "-0.377**", "-1.007*", 
        "-0.722***", "-0.347***"
    ),
    ma2 = c(
        " ", " ", "-0.047", " ", "0.013", "-0.046",
        "0.004", "0", "0", " ", " ", "0", "0", " ", " "
    ),
    ma3 = c(
        " ", " ", " ", " ", "-0.217***", " ",
        "-0.157**", "-0.214***", "-0.156**", 
        " ", " ", "-0.152**", "0.193", " ", " "
    ),
    sar1 = c(
        " ", " ", " ", " ", " ", " ", " ", " ", " ",
        " ", "-0.031", " ", " ", " ", " "
    ),
    sma1 = c(
        " ", "-0.890***", " ", "-0.841***", " ",
        "-0.839***", "-0.838***", " ", "-0.838***",
        "-0.839***", "-0.831***", "-0.839***",
        "-0.846***", "-0.841***", "-1.196***"
    ),
    AICc = c(
        "279.55", "154.53", "280.87", "123.84", "270.05", 
        "125.30", "119.57", "268.05", "117.51", "123.37",
        "125.69", "119.55", "123.1", "121.15", "119.96"
    )
) |> kable()
```



### Pure Moving Average Models

We begin by fixing our AR parameters $p= 0$ and $P= 0$ and determine the optimal pure Moving Average (MA) model. From our ACF and PACF plots we determined that our model should include up to $q= 3$ MA terms and $Q= 1$ seasonal MA terms. We proceed to fit various models with the results of the fitting procedure for our top performing models, shown below in Tables 3 and 4.

```{r}
#| label: tbl-maperformance
#| tbl-cap: "MA model performance criteria."

data.frame(
    Model = c("SARIMA(0,1,3)(0,1,1)12", "SARIMA(0,1,3)(0,1,1)12 (MA2=0)"),
    AIC = c(119.38, 117.38),
    AICc = c(119.57, 117.51),
    BIC = c(138.08, 132.34)
) |>
    kable()
```

