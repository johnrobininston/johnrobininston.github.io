{
  "hash": "2e7457e4f56f798c8fae32995e0bd389",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Markov Chains\nsubtitle: Markov property, transition probabilities, state distributions, chain classification, Chapman-Kolmogorov equations and limiting distributions.\nauthor: John Robin Inston\ndate: \"2026-02-12\"\ndate-modified: \"2026-02-12\"\ncategories: [Stochastic Processes]\ntoc: true\ntitle-block-banner: true\n---\n\n\n\n## Markov Chains\n\n### Markov Property\n\nIn probability theory a stochastic process is said to satisfy the <mark>_Markov property_</mark> if its future evolution is <mark>_independent of its history_</mark>, that is, the process is <mark>_memoryless_</mark>.  Consider a discrete time stochastic process $\\{X_t\\}_{t\\in\\mathbb{N}}$ defined in the probability space $(\\Omega,\\mathcal{F}, \\mathbb{P})$.\n\n> **Definition:** (<mark>**Markov Property**</mark>) \\\n> The discrete time stochastic process $\\{X_t\\}_{t\\in\\mathbb{N}}$ is said to satisfy the _Markov property_ if $$\\mathbb{P}(X_{n+1}=x_{n+1}|X_n=x_n, ..., X_1=x_1)=\\mathbb{P}(X_{n+1}=x_{n+1}|X_n=x_n).$$\n\nIntuitively, we see that we gain no additional information about the probability of the next value of the process by conditioning on the entire history rather than the latest value.  Any process satisfying the markov property is called a <mark>**Markov process**</mark>. \n\n### Markov Chains\n\nA Markov process with a discrete state space is known as a <mark>**Markov chain (MC)**</mark>.  Assume that $\\{X_t\\}_{t\\in\\mathbb{N}}$ is a Markov chain with finite state space $\\mathcal{X}:=\\{1, ..., M\\}$.  The evolution of the chain in time is described by <mark>**transition probabilities**</mark> $p_{i,j}^{(n)}(t)$ which give the probability that the chain is in state $j$ at time $t+n$ given that the chain was in $i$ at time $t$ \n$$\np_{i,j}^{(n)}(t)=\\mathbb{P}(X_{t+n}=j|X_{t}=i).\n$$\n\nWe summarize the transition probabilities in an $M\\times M$ <mark>**transition probability matrix**</mark> denoted by \n$$\nP^{(n)}(t)=(p_{i,j}^{(n)}(t))_{i,j\\in\\mathcal{X}}.\n$$\n\nThe transition probability matrix is <mark>_stochastic_</mark> which means that for all $n,t\\in\\mathbb{N}$ we have:\n\n1. $0\\leq p_{i,j}^{(n)}(t)<1$ for all $i,j\\in \\mathcal{X}$,\n2. $\\sum_{{j\\in S}}p_{{i,j}}^{(n)}(t)=1$ for all $i\\in \\mathcal{X}$ (the rows sum to 1).\n\nTo simplify notation we denote the <mark>_one-step transition probablity matrix_</mark>  \n$$\nP(t)=(p_{i,j}(t))_{i,j\\in\\mathcal{X}}.\n$$\n\nThe transition probabilities are <mark>_stationary_</mark> when they are invariant in time, that is\n$$\np_{i,j}^{(n)}(t)=p_{i,j}^{(n)}(s);~~\\forall s,t\\in\\mathbb{N}.\n$$\n\nWe say that a MC is <mark>**homogenous**</mark> if it has stationary transition probabilities.  For homogenous MCs we are often interested in the <mark>one-step transition probabilities</mark> and so again to clarify notation we denote the one-step transition probability matrix for homogenous MCs as\n$$\nP=(p_{i,j})_{i,j\\in\\mathcal{X}}.\n$$\n\n**Example:** Consider a homogenous MC $\\{X_t\\}$ with finite state space $\\mathcal{S}:=\\{1,2,3\\}$ with transition probability matrix\n$$\nP = \\begin{bmatrix}\n0 & 0.6 & 0.2 & 0.2 \\\\\n0.75 & 0 & 0.25 & 0 \\\\\n0.25 & 0.25 & 0.25 & 0.25 \\\\\n0.25 & 0 & 0.25 & 0.5\n\\end{bmatrix}.\n$$\n\n### Transition Diagrams\n\nTo gain deeper insight into the behavior of the chain we typically will produce <mark>*transition probability diagram*</mark> which takes the form of a directed graph with nodes denoting states and weighted directed arrows the transition probabilities.  The transition probability diagram for our example is\n\n![Transition probability diagram.](assets/transition-probability-diagram.png){width=50%}\n\nThere are several alternate versions of this diagram including the <mark>_integer weighted diagram_</mark>. To construct the integer-weighted diagram we compute the inter-weighted matrix by multiplying elements of each row by the lowest common multiple for that row.  The integer weighted matrix for our example is\n$$\nP = \\begin{bmatrix}\n0 & 3 & 1 & 1 \\\\\n3 & 0 & 1 & 0 \\\\\n1 & 1 & 1 & 1 \\\\\n1 & 0 & 1 & 2\n\\end{bmatrix}.\n$$\n\nThe corresponding diagram is\n\n![Integer-weighted transition probability diagram.](assets/iw-transition-probability-diagram.png){width=50%}\n\nSince our diagram has a symmetric integer-weighted transition matrix we can also write the <mark>_non-directed version_</mark> by combining the equally weighted directed arrows into a single non-directed line\n\n![Integer-weighted non-directed ransition probability diagram.](assets/iwnd-transition-probability-diagram.png){width=50%}\n\n### State Distributions\n\nThe <mark>**state distribution**</mark> $\\mu_i(t)$ gives the probability that the chain is in state $i$ at time $t$ \n$$\n\\mu_i(t)=\\mathbb{P}(X_{t}=i).\n$$\n\nFor convenience we summarize the state distributions in vector notation\n$$\n\\boldsymbol{\\mu}(t)=(\\mu_i(t))_{i \\in \\mathcal{X}}.\n$$\n\nOf particular interest is the <mark>**initial state distribution**</mark> \n$$\n\\boldsymbol{\\mu}(0)=\\boldsymbol{\\mu}=(\\mu_i(0))_{i \\in\\mathcal{X}}.\n$$ \n\nFor conveinence we also introduce the following notation for process vectors \n$$\nX_{1:t} = (X_1, ..., X_t),\n$$\n\nwhere similarly $X_{1:t}=x_{1:t}$ is equivalent to $X_1=x_1, ..., X_t=x_t$.\n\n<mark>_Finite-dimensional distributions_</mark> allow us to evaluate the behavior of the distribution of infinite length processes by selecting some sub-sequence of index values to evaluate their joint distribution.\n\n> **Proposition (<mark>Finite-Dimensional Distributions</mark>):** \\\n> The *finite-dimensional distributions* of $X=(X_{n})_{n}$ are determined by the initial mass $\\mu^{(0)}$ and transition matrix $P$ where $$P(X_{0}=i_{0}, \\dots, X_{n}=i_{n})=\\mu_{{x_{0}}}^{(0)}p_{x_{0}, x_{1}}(0)p_{x_{1}, x_{2}}(1)\\cdots p_{x_{n-1}, x_{n}}(n-1).$$\n\n**Proof:** \\\nFrom application of the Chain Rule and the Markov Property we see that\n$$\n\\begin{align}  \n\\mathbb{P}&(X_{1 : n}=x_{1: n}) & \\\\\n & = \\mathbb{P}(X_{0}=x_{0}) \\mathbb{P}(X_{1}=x_{1}|X_{0}=x_{0})\\mathbb{P}(X_{2}=x_{2}|X_{0 : 1}=x_{0 : 1})\\cdots \\\\\n & \\quad \\cdots \\mathbb{P}(X_{n}=x_{n}|X_{0 : n-1}=x_{0: n-1})  \\\\\n  & =\\mathbb{P}(X_{0}=x_{0})\\mathbb{P}(X_{1}=x_{1}|X_{0}=x_{0})\\mathbb{P}(X_{2}=x_{2}|X_{1}=x_{1})\\cdots \\\\\n  & \\quad \\cdots \\mathbb{P}(X_{n}=x_{n}|X_{n-1}=x_{n-1})  \\\\\n& = \\mu_{{x_{0}}}^{(0)}p_{x_{0}, x_{1}}(0)p_{x_{1}, x_{2}}(1)\\cdots p_{x_{n-1}, x_{n}}(n-1),\n\\end{align}\n$$\n\nas required. \\\n$\\square$\n\n> **Proposition: (<mark>Multi-step Transition Probabilities for homogenous MCs</mark>)** \\\n> Let $\\{ X_{n} \\}$ be a homogenous MC with one-step transition probability matrix $P$. The $m$-step transition probabilities are given by raising the one-step transition probability matrix to the $m$-th power $$P^{(m)}=(\\mathbb{P}(X_{m}=j|X_{0}=i))_{i,j}=P^m.$$\n\n**Proof:** \\\nThe result follows immediately from the Markov property since\n$$\n\\begin{aligned}\n\\mathbb{P}(X_{m}|X_{0}) & =\\mathbb{P}(X_{m}|X_{m-1})\\mathbb{P}(X_{m-1}|X_{0}) \\\\\n& = \\qquad \\vdots \\\\\n& = \\mathbb{P}(X_{m}|X_{m-1})\\cdots \\mathbb{P}(X_{1}|X_{0}) \\\\\n& = [\\mathbb{P}(X_{1}|X_{0})]^m.\n\\end{aligned}\n$$\n\n$\\square$\n\n<em>**Example 1:** Consider a homogenous MC with state space $\\mathcal{S}:=\\{ 1,2,3 \\}$ with transition probability matrix $P$ and initial state distribution $\\mu$ given respectively by\n$$\nP:=\\begin{bmatrix}\n0.4 & 0.6 & 0 \\\\\n0.7 & 0.3 & 0  \\\\\n0 & 0.1 & 0.9\n\\end{bmatrix},~~\\mu=\\begin{bmatrix}\n0.1 & 0.2 & 0.7\n\\end{bmatrix}.\n$$\n\nThe transition probability diagram for this chain is \n\n![Transition Probability Diagram.](assets/state-diagram.jpeg){width=50%}\n\nSuppose we wish to determine the $3$-step transition probabilities.  Applying Proposition 1 we can compute $\\mathbb{P}(X_3|X_0)$ as $P^3$:\n\n::: {#f2544d69 .cell execution_count=2}\n``` {.python .cell-code}\nP = np.array([\n    [.4, .6, 0],\n    [.7, .3, 0],\n    [0, .1, .9]\n])\nP3 = np.linalg.matrix_power(P,3); print(P3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[0.526 0.474 0.   ]\n [0.553 0.447 0.   ]\n [0.112 0.159 0.729]]\n```\n:::\n:::\n\n\nFurthermore, let us suppose that we wish to determine the 3-step state distribution.  From the finite-dimensional distribution we see that\n$$\n\\begin{aligned}\n\\mathbb{P}(X_3) & =\\mathbb{P}(X_0)\\mathbb{P}(X_1|X_0)\\mathbb{P}(X_2|X_1)\\mathbb{P}(X_3|X_2) \\\\\n & = \\boldsymbol{\\mu}P^3.\n\\end{aligned}\n$$\n\nThus we can compute:\n\n::: {#97eaefd0 .cell execution_count=3}\n``` {.python .cell-code}\nmu = np.array([\n    0.1, 0.2, 0.7\n])\nmu@(P3)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\narray([0.2416, 0.2481, 0.5103])\n```\n:::\n:::\n\n\n</em>\n\n## MC Classification\n\n### State Classification\n\nWe can classify MC states based on how they appear in the chain and their behavior in the limit.  States can be _persistent_ (_recurrent_), _null-persistent_, _transient_, _absorbing_, _periodic_ and _ergodic_.\n\n> **Definition: (<mark>Persistent States</mark>)**\n> A state $j$ is _persistent_ if the probability that the process will return to $j$ given that it started at $j$ eventually is 1, that is $$\\mathbb{P}(X_{n}=j~\\text{for some}~n\\geq 1|X_{0}=j)=1.$$\n\nA sub-class of persistent states are <mark>_null-persistent states_</mark> which are persistent thats that have an infinite mean recurrence time.  States that are not null-persistent are said to be <mark>_positive_persistent_</mark>.\n\nFor example, consider a symmetric random walk. Any point in the state space is null-persistent since the random walk can always return but may take infinitely long to do so. We discuss null-persistent states in a later note on MCs.\n\n> **Definition: (<mark>Transient States</mark>)** \\\n> Alternatively, if a state $j$ is not persistent it must be _transient_, that is, the probability that the process will return to $j$ given that it started at $j$ eventually is 0 i.e. the processes structure prevents it from returning.  This is equivalent to writing $$\\mathbb{P}(X_{n}=i~\\text{for some}~n\\geq 1|X_{0}=i)<1.$$\n\n> **Definition: (<mark>Periodic States</mark>)** \\\n>  The *period* of a state $j$ is the greatest common divider of all $n$ for which $p_{{i,i}}(n)>0$ i.e. \n> $$d(i)=gcd\\{n:p_{i,i}(n)>0\\}.$$\n> If $d(i)=1$ then the state $j$ is *aperiodic* and otherwise, the state is said to be *periodic*.\n\n> **Definition: (<mark>Ergodic</mark>)** \\\n> A state $j$ is <mark>_ergodic_</mark> if it is positive persistent and aperiodic i.e. $\\mu_{i,i}<\\infty$ and $d(i)=1$.\n\n> **Definition: (<mark>Absorbing</mark>)** \\\n> A state $j$ is <mark>_absorbing_</mark> if $p_{j,j}=1$, that is the probability of leaving state $j$ once the process has entered is 0. \n\n### Chain Classification\n\nTo classify chains we first define state <mark>_communication_</mark>, that is when paths between states have non-zero probability.\n\n> **Definition: (<mark>Communicating States</mark>)** \\\n> For MC $\\{ X_{t} \\}$ state $i$ _communicates_ with state $j$, denoted $i \\to j$, if $p_{i,j}(n)>0$ for some $n$.  If $i\\to j$ and $j \\to i$ we say that $i$ and $j$ _intercommunicate_, denoted $i \\leftrightarrow j$.\n\nWe can show that intercommunication is an <mark>_equivalence class_</mark> and thus can be used to split MC state spaces into communication classes.  For $i,j$ in the same communication class we have:\n\n1. States $i$ and $j$ have the same period;\n\n2. State $i$ is transient iff $j$ is transient; and\n\n3. State $i$ is null persistent iff $j$ is.\n\n> **Definition: (<mark>Irreducibility</mark>)** \\\n> A set of states $C$ is <mark>_irreducible_</mark> if for all $i,j\\in C$, $i\\leftrightarrow j$, that is all states within $C$ inter-communicate.\n\n> **Definition: (<mark>Closed</mark>)** \\\n> A set of states $C$ is <mark>_closed_</mark> if for all $i \\in C$, $p_{i,j}=0$ for all $j\\not\\in C$, that is, the chain never leaves $C$ once it has entered.  Not that clearly the set consisting of one absorbing state is closed.\n\n<em>\n\n**Example:** Consider the MC transition diagram shown in the figure below.\n\n![Example transition probability diagram.](assets/complicated-transition-graph.png){width=50%}\n\nWe define $S_1:=\\{5\\}$, $S_2:=\\{1,2,3,4\\}$ and $S_3:=\\{6,7\\}$.  The chain is clearly reducible since $2\\to 5$ and $3\\to 6$ are one way journies.  We can see that $5$ is absorbing since the chain can never leave once it arrives.  We see that states $1,2,3,4$ are transient since the chain will visit them and never return at some point, and states $6$ and $7$ are recurrent since the chain will always revisit them.  We say that $S_1$ is an closed recurrent (absorbing) class, $S_2$ is a transient class and $S_3$ is a closed recurrent class.\n\n</em>\n\n## Chapman Kolmogorov Equations\n\nThe <mark>_Chapman-Kolmogorov Equations (CKE)_</mark> relate the joint probability distributions of different sets of coordinates on a stochastic process.  \n\n> **Theorem 2: (<mark>Chapman-Kolmogorov Equations</mark>)** \\\n> For a discrete time countable state homogeneous Markov chain the Chapman-Kolmogorov equations state that \n> $$P_{n+m}=P_{n}\\cdot P_{m},$$\n> or equivalently \n> $$p^{(n+m)}_{i,j}=\\sum_{k\\in S}p^{(n)}_{i,k}p^{(m)}_{k,j}.$$\n\nA similar result holds for distributions\n$$\n\\mu_{j}^{(n+m)}=\\sum_{i \\in S}\\mu_{i}^{(n)}p_{i,j}(m),\n$$\n\nor in matrix form \n$$\n\\boldsymbol{\\mu}^{(n+m)}=\\boldsymbol{\\mu}^{(n)}P_{m}=\\boldsymbol{\\mu}^{(n)}P^m\\quad\\&\\quad \\boldsymbol{\\mu}^{(n)}=\\boldsymbol{\\mu}^{(0)}P^n.\n$$\n\n## Limiting Distributions\n\n### Limiting Distribution\n\nConsider the long-term behavior of a MC $\\{ X_{n} \\}_{n=0}^\\infty$ when $n \\to \\infty$.  It is possible for the chain to converge to a particular state (e.g. a Galton-Watson-Bienaymé (GWB) Branching Process can converge to 0).  Additionally, it is possible for a MC to converge to some random variable $X~a.s.$ as $n\\to \\infty$.  Intuitively, if a Markov chain runs for a long time it generally doesn't converge because it is always jumping around but its distribution can settle down.  \n\n> **Definition: (<mark>Limiting Distribution</mark>)**\n> The _limiting distribution_ of a homogenous MC with initial distribution $\\boldsymbol{\\mu}$ and TP matrix $P$ is the distribution vector\n> $$\\lambda:=\\lim_{n\\to\\infty}\\mu\\cdot P^n.$$\n\nThis is often a challenging limit to find analytically and even numerically often requires Monte-Carlo simulations.  \n\n<em>\n**Example:** Reconsider our last MC example with transition diagram given in the figure below.\n\n![Complicated transition diagram.](assets/complicated-transition-graph.png){width=50%}\n\nWe know that the $S_2:=\\{1,2,3,4\\}$ is transient and so given enough time to run, the chain will either end on in $S_1=\\{5\\}$ or $S_3=\\{6,7\\}$.  We therefore need to ask the following questions:\n\n1. How do we know what proportion of the time the chain ends in each class. \n2. If the chain ends up in closed irreducible $S_3$, what is the behavior of the state distribution in the limit? $\\rightarrow$ <mark>_Stationary Distributions_</mark>\n\n</em>\n\n### Stationary Distribution\n\nA process is <mark>**strictly stationary**</mark> if its distribution does not change under translations, i.e. over time.  More formally we give the following definition.\n\n> **Definition: (<mark>Strictly Stationary Process</mark>)** \\\n> A process $\\{ X_{n},~n\\geq 0 \\}$ is _strictly stationary_ if for any integers $m\\geq 0$ and $k>0$, we have \n> $$(Y_{0}, Y_{1}, \\dots Y_{m})\\stackrel{\\mathcal{D}}{=}(Y_{k}, Y_{k+1}, \\dots, Y_{k+m})$$ \n> that is, the distribution does not change under translations.\n\nThis is often a challenging condition to show and so we also define a <mark>**weak stationarity**</mark> of the mean and covariance being invariant to changes in time.  See oour discussion of Time Series Anaysis for more detailed noted on stationarity.\n\n> **Definition: (<mark>Stationary Distribution</mark>)** \\\n> The vector $\\pi=(\\pi_{j},~j \\in S)$ is called a _stationary distribution_ of a Markov chain if:\n> \n> 1. $\\pi_{j}\\geq 0$.\n> 2. $\\sum_{j\\in S}\\pi_{j}=1$.\n> 3. $\\pi=\\pi P$.\n\nNote that (3) can equivalently be written as\n$$\n\\pi_{j}=\\sum_{i \\in S}\\pi_{i}p_{i,j}\\text{ for all }j \\in S.\n$$\n\nAlso note that $\\pi P^2=\\pi P\\cdot P=\\pi P=\\pi$ and similarly, for all $n>1$, $\\pi P^n=\\pi$, that is $\\pi_{j}=\\sum_{i\\in S}\\pi_{i}p_{i,j}(n)$ for all $j \\in S$.\n\n> **Theorem: (Stationary Distribution)**\n> For any irreducible MC $\\{X\\}$ with a finite state space $\\mathcal{X}$ there exists a stationary distribiton $\\pi$.\n\n*Proof:*\n\nLet $P$ be the transition matrix on a finite state space $\\mathcal{X}$, and assume the chain is irreducible.  In a finite Markov chain, at least one state is recurrent (otherwise all states would be transient, which is impossible in a finite state space because probability mass would have to “escape” forever without accumulating anywhere).  Since the chain is irreducible, all states communicate with that recurrent state, hence every state is recurrent.\n\nBecause the chain is finite and irreducible, starting from any state $i$, every other state $j$ is hit with positive probability in at most $|S|-1$ steps along some path. In particular, there exists an $m\\ge 1$ with\n$$\np^{(m)}_{ii} > 0.\n$$\n\nLet $\\tau_i^+ = \\inf\\{n\\ge1: X_n=i\\}$ be the first return time to $i$.  Then each block of $m$ steps has probability at least $p^{(m)}_{ii}$ of containing a return to $i$ at its endpoint, so\n$$\n\\mathbb{P}_i(\\tau_i^+ > km) \\le (1-p^{(m)}_{ii})^k.\n$$\n\nHence the tail of $\\tau_i^+$ is geometrically bounded, implying\n$$\n\\mathbb{E}_i[\\tau_i^+] < \\infty.\n$$\n\nSo the chain is positive recurrent.  Define, for each $j\\in S$,\n$$\n\\pi_j := \\frac{\\mathbb{E}_i\\!\\left[\\sum_{n=0}^{\\tau_i^+-1}\\mathbf{1}\\{X_n=j\\}\\right]}{\\mathbb{E}_i[\\tau_i^+]}.\n$$\n\nThis is the expected fraction of time spent in state $j$ during an $i\\to i$ regeneration cycle. Clearly $\\pi_j\\ge 0$ and\n$$\n\\sum_{j\\in S}\\pi_j\n= \\frac{\\mathbb{E}_i\\!\\left[\\sum_{n=0}^{\\tau_i^+-1}\\sum_{j\\in S}\\mathbf{1}\\{X_n=j\\}\\right]}{\\mathbb{E}_i[\\tau_i^+]}\n= \\frac{\\mathbb{E}_i[\\tau_i^+]}{\\mathbb{E}_i[\\tau_i^+]}=1,\n$$\n\nso $\\pi$ is a probability distribution.  To show $\\pi P=\\pi$, note that the expected number of visits to $j$ during a cycle equals the expected number of transitions *into* $j$ during the cycle (up to the regeneration boundary, which contributes no net imbalance because the cycle starts and ends at $i$). Formally, by the Markov property and counting transitions within the cycle,\n$$\n\\mathbb{E}_i\\!\\left[\\sum_{n=0}^{\\tau_i^+-1}\\mathbf{1}\\{X_{n+1}=j\\}\\right]\n= \\sum_{k\\in S}\\mathbb{E}_i\\!\\left[\\sum_{n=0}^{\\tau_i^+-1}\\mathbf{1}\\{X_n=k\\}\\right]p_{kj}.\n$$\n\nDivide both sides by $\\mathbb{E}_i[\\tau_i^+]$ to get\n$$\n\\pi_j = \\sum_{k\\in S}\\pi_k p_{kj},\n$$\n\nwhich is exactly $\\pi P=\\pi$. Thus a stationary distribution exists. \\\n$\\square$\n\n<em>**Example:** Consider a Markov chain describing the meals served by a restaurant with transition graph shown below in Figure 1 provided by helpful video on Markov chains by [Normalized Nerd](https://www.youtube.com/watch?v=i3AkTO9HLXo&t=110s).  \n\n![Example transition diagram for a MC describing meals served at a restaurant.](assets/food_example.png){width=50%}\n\nTaking state 1 to be hamburger, state 2 to be pizza and state 3 to be hotdog, the transition matrix $P$ can be written as\n$$\nP=\\begin{bmatrix}\n0.2 & 0.6 & 0.2  \\\\\n0.3 & 0 & 0.7  \\\\\n0.5 & 0 & 0.5 \n\\end{bmatrix}.\n$$\n\nGiven the restaurant first serves pizza we can define the initial distribution $\\pi_{0}=\\begin{bmatrix}0 & 1 & 0\\end{bmatrix}$.  Applying the transition matrix $P$ we get\n$$\n\\pi_{0}P=\\begin{bmatrix}\n0 & 1 & 0\n\\end{bmatrix}\\cdot \\begin{bmatrix}\n0.2 & 0.6 & 0.2  \\\\\n0.3 & 0 & 0.7  \\\\\n0.5 & 0 & 0.5 \n\\end{bmatrix}=\\begin{bmatrix}\n0.3 & 0 & 0.7\n\\end{bmatrix}=\\pi_{1},\n$$\n\nthe second state future transition probabilities.   Repeating this step for $\\pi_{1}$ we have\n$$\n\\pi_{0}P=\\begin{bmatrix}\n0.3 & 0 & 0.7\n\\end{bmatrix}\\cdot \\begin{bmatrix}\n0.2 & 0.6 & 0.2  \\\\\n0.3 & 0 & 0.7  \\\\\n0.5 & 0 & 0.5 \n\\end{bmatrix}=\\begin{bmatrix}\n0.41 & 0.18 & 0.41\n\\end{bmatrix}=\\pi_{2},\n$$\n\nIf a stationary distribution $\\pi$ exists it would mean that as $\\pi_{0}, \\pi_{1}, \\dots$ continues, eventually it will reach a point where it doesn't change when $P$ is applied, hence using linear algebra we can write the expression\n$$\n\\pi P=\\pi,\n$$\n\nsee Definition 2 above.  Additionally, since $\\pi$ is a vector of probabilities we have that $\\pi(1)+\\pi(2)+\\pi(3)=1$ and solving this system gives the stationary distribution\n$$\n\\pi=\\begin{bmatrix}\n\\frac{25}{71} & \\frac{15}{71} & \\frac{31}{71}\n\\end{bmatrix}.\n$$\n\nWe can numerically confirm our result by iteratively applying the transition probability matrix:\n</em>\n\n::: {#777e96f5 .cell fig-height='6' fig-width='12' execution_count=4}\n``` {.python .cell-code}\nP = np.array([\n    [.2, .6, .2],\n    [.3, 0, .7],\n    [.5, 0, .5]\n])\npi = np.array([\n    0, 1, 0\n])\npi_list = []\npi_list.append(pi)\n\nn = 50\n\nfor _ in range(n):\n    pi = pi @ P\n    pi_list.append(pi) \n\npi_list = np.array(pi_list).squeeze()\nfig, ax = plt.subplots()\nax.plot(range(n+1), pi_list[:,0], color=\"red\", label=\"Hamburger\")\nax.plot(range(n+1), pi_list[:,1], color=\"green\", label=\"Pizza\")\nax.plot(range(n+1), pi_list[:,2], color=\"blue\", label=\"Hotdog\")\n```\n\n::: {.cell-output .cell-output-display}\n![Transition probability convergence plot.](index_files/figure-html/cell-5-output-1.png){width=571 height=411 fig-align='center'}\n:::\n:::\n\n\n**Definition: (<mark>Doubly Stochastic Matrices</mark>)** \\\nA matrix $P:=(p_{i,j})_{i,j\\in\\mathcal{S}}$ is said to be doubly stochastic when both rows and columns sum to 1, that is\n$$\n\\sum_{j\\in\\mathcal{X}}p_{i,j} = \\sum_{i\\in\\mathcal{X}}p_{i,j}=1.\n$$\n\n**Theorem: (<mark>Uniform Stationary Distributions for Doubly Stochastic MCs</mark>)** \\\nA finite state homogenous MC with doubly stochastic transition probability matrix has a uniform stationary distribution.\n\n*Proof* \\\nAssume $\\mathcal{X}$ has $n$ elements and define $\\pi = (\\pi_i)_{i\\in\\mathcal{X}}$ where $\\pi_i=1/n$.  Then clearly\n\n$$\n\\sum_{i\\in\\mathcal{X}} \\pi_i p_{i,j} = \\frac{1}{n}\\sum_{i\\in\\mathcal{X}}p_{i,j} = \\frac{1}{n} = \\pi_j,\n$$\n\nholds for all $j\\in\\mathcal{X}$.  Since by definition $\\pi$ is normalized we obtain the result. \\\n$\\square$\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}