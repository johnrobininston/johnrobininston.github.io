{
  "hash": "f17fe529f31de5a03a7c30cea894855b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Neural Networks with Python\nauthor: John Robin Inston\ndate: today\ncategories: [Python, Machine Learning]\ndraft: false\ntoc: true\njupyter: python3\n---\n\nIn this note how build our first neural network in python using the guide\n[Python AI: How to Build a Neural Network & Make Predictions](https://realpython.com/python-ai-neural-network/) provided by Real Python.  We construct a simple network from scratch rather than using libraries such as `scikit-learn`, `pytorch` or `tensorflow` which we cover in a different note.\n\n## Background\n\n### Machine Learning\n\n**Machine learning (ML)** involves solving a problem by training a statistical model rather than explicitly programming rules.  There are two types of ML:\n\n1. **Supervised Learning** - considers *labelled data* meaning inputs have a corresponding correct output where the goal of the training is to predict outputs for new unseen inputs (e.g. classification, regression),\n   \n2. **Unsupervised Learning** - considers *unlabelled data* with no explicit output labels where the goal of the training is to uncover hidden relationships in the data.\n\nWe call input data **features** and **feature engineering** is the process of extracting features from raw data, i.e. representing different kinds of data in order to extract meaningful information.  \n\n:::{.callout-tip}\n\nFor example, **lemmatization** is the process of removing inflection from words in a sentence i.e. sleeping, sleeper, sleepiest all become sleep.  This helps reduce data sparsity (variety of values) and increase algorithm performance. \n\n:::\n\n### Deep Learning \n\n**Deep learning (DL)** involves allowing the ML model to determine important features by itself rather than manually applying feature engineering techniques.  This is ideal for complex data where feature engineering quickly becomes untractable - e.g. extracting data to predict a person's mood from a photograph of their face.\n\n### Neural Networks\n\nA **Neural Network (NN)** is a ML model inspried by the structure and function of the human brain built from layers of interconnected units called **neurons** arranged into layers.  We think of a neural network as essentially a *layered function approximator* whose general model structure has the following components:\n\n1. **Input Layer** - takes in raw data (e.g. image pixels, text, numerical features),\n\n2. **Hidden Layers** - performs transformations and extracts patterns using weighted connections and nonlinear activation functions (e.g. `ReLU`, `sigmoid`, `tanh`),\n\n3. **Output Layer** - outputs predictions.\n\nEssentially, you can think of each layer transformation as a feature extraction step as we are pulling some representation of the data that came previously.\n\n![Figure: Neural network diagram.](static/img/neural-network.png){width=\"70%\"}\n\nLike the human brain, NN work by neurons firing (activating) which sends signals down synapses (connections) to activate neighboring neurons.  Each connection has a **weight** that determines how strongly one neuron influences another.  Neurons also have a **bias** term to shift activation and improve model flexibility (i.e. how activated the neuron is).  There are several types of neural networks including:\n\n1. **Feedforward Neural Networks (FNNs):** Basic form, information moves forwards from input to output.\n\n2. **Convolution Neural Networks (CNNs):** Specialized for images and spatial data.\n\n3. **Recurrent Neural Networks (RNNs):** Handle sequential data (e.g. text, time series).\n\n4. **Transformers:** Modern architecture for language and sequence tasks (e.g. GPT).\n\nIn this note we will consider FNNs. To train a NN we start with some random weights and bias values before using the network to make a prediction.  This is then compared to the desired output and the vectors are adjusted to improve prediction accuracy.  \n\n### Activation Functions\n\nThe **activation functions** of the NN are there to introduce non-linearity.  Each neurons connections are simply given by a weighted sum of other neurons hence a neural network is essentially a composition of linear maps which will just collapse to a single linear transformation since the composition of linear transformations is itself linear.  \n\nActivation functions allow us to introduce non-linearity such that each layer can warp the input space in a way that lets later layers combine them into much more expressive shapes.  Activation functions are chosen to be non-linear, almost everywhere differentiable and computationally efficient.  Some of the most popular include:\n\n1. `sigmoid` - outputs in (0,1), good for probabilities in binary classification but suffers from vanishing gradient problem;\n2. `tanh` - outputs in (–1,1), useful when you want centered data; and\n3. `ReLU` - unbounded above and zero below, great for general deep nets.\n\n### Backpropogation\n\nAs you can imagine, with multiple neurons over multiple layers the neural network will have a large number of parameters that we are looking to fit.  When training a neural network we update these parametes using backpropogation which includes:\n\n1. **Forward pass:** Feed an input $x$ through the network with parameters $\\theta$ $\\rightarrow$ to obtain output $\\hat{y}$,\n2. **Compute loss:** Compare $\\hat{y}$ to the true label $y$ using the loss function $L$,\n3. **Backwards propogation:** Use the chain rule to compute $L_{\\theta}$,\n4. **Update:** Improve model fit by updating parameters using **gradient decent**.\n\n**Gradient descent** is the procedure whereby we look for the parameters that minimize the loss by using the gradient as a compass. Intuitively, the loss for a realization of a model is a function in the input parameters $\\theta$ thus can be interpretted as a high-dimensional surface with peaks and valleys.  We are looking for the parameters coordinates that correspond to the lowest point and use the slope of the point we are currently at as our guide.  That is, once we compute the gradient of the loss with respect to the various parameters we then change the parameters by subtracting the gradient.\n\nThere are many problems with gradient descent including becoming stuck in local minima, learning rate problems, vanishing or exploding gradients and poorly conditioned problems but we defer our discussion of gradient descent to a later note.  However we note that it is necessary to include a **learning rate** term $\\alpha$ which reducing the size of the parameter update to a much smaller proportion of the derivative (e.g. common values are $\\alpha=0.1, 0.01, 0.001$ etc.).  This is to avoid bouncing back and forth and missing the minimum points below.\n\n## Neural Networks in Python\n\nWe will be constructing our NN in `python` and requires the following packages:\n\n::: {#183b6bde .cell execution_count=1}\n``` {.python .cell-code}\n# Packages\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n### Basic Example - 1 layer binary classification\n\nOur first network is a single-neuron (no hidden layers) neural network (effectively logistic regression with a sigmoid on a 2-dimensional input). For an input $x\\in\\mathbb{R}^2$, weights $w\\in\\mathbb{R}^2$, and bias $b\\in\\mathbb{R}$, it computes\n$$\n\\begin{align}\nz & = w^{\\top}x+b \\\\\n\\hat{y} & = \\sigma(z)\n\\end{align}\n$$\n\nwith output $\\hat{y}$ and denoting the sigmoid function \n$$\n\\sigma=\\frac{1}{1+\\exp(-x)}.\n$$\n\nFor future reference the derivative of the sigmoid function can be written as\n$$\n\\begin{align}\n\\frac{d\\sigma}{dx} & = \\frac{\\exp(-x)}{(1+\\exp(-x))^{2}} \\\\\n& = \\frac{-1+1+\\exp(-x)}{(1+\\exp(-x))^2} \\\\\n& = \\frac{-1}{(1+\\exp(-x))^2} + \\frac{1}{1+\\exp(-x)} \\\\\n& = \\frac{1}{1+\\exp(-x)}\\left(-\\frac{1}{1+\\exp(-x)}+1\\right) \\\\\n& = \\sigma(x)(1-\\sigma(x)).\n\\end{align}\n$$\n\nWe use the mean-squared error (MSE) for a single example: \n$$\nL=(\\hat{y}−y)^2.\n$$\n\nWe update parameters with plain gradient descent where we compute gradients with the chain rule.  Denoting parameters $\\theta=(w,b)$ from the chain rule we have that\n$$\n\\begin{align}\n& \\frac{dL}{d\\theta} = \\frac{dL}{d\\hat{y}}\\cdot \\frac{d\\hat{y}}{dz}\\cdot \\frac{dz}{d\\theta} \\\\\n\\implies &\n\\begin{cases} \\\n    \\frac{dL}{dw} & = 2(\\hat{y}-y)\\cdot \\sigma(1-\\sigma) \\cdot w \\\\\n    \\frac{dL}{db} & = 2(\\hat{y}-y)\\cdot \\sigma(1-\\sigma).\n\\end{cases}\n\\end{align}\n$$\n\nGradients are computed by the chain rule and parameters are updated by plain gradient descent.  We define the following `NeuralNetwork` python class:  \n\n::: {#e1685301 .cell execution_count=2}\n``` {.python .cell-code}\nclass NeuralNetwork:\n\n    # initialization \n    def __init__(self, alpha):\n        self.weights = np.array([np.random.randn(), np.random.randn()])\n        self.bias = np.random.randn()\n        self.alpha = alpha\n    \n    # sigmoid function\n    def _sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n    \n    # sigmoid derivative function\n    def _sigmoid_deriv(self, x):\n        return self._sigmoid(x) * (1 - self._sigmoid(x))\n    \n    # predict function\n    def predict(self, input_vector):\n        layer_1 = np.dot(input_vector, self.weights) + self.bias\n        layer_2 = self._sigmoid(layer_1)\n        prediction = layer_2\n        return prediction\n    \n    # function computing the gradient\n    def _compute_gradients(self, input_vector, target):\n        # computing layers and prediction\n        layer_1 = np.dot(input_vector, self.weights) + self.bias\n        layer_2 = self._sigmoid(layer_1)\n        prediction = layer_2\n        # computing \n        dL_dp = 2 * (prediction - target)\n        dp_dz = self._sigmoid_deriv(layer_1)\n        dz_db = 1\n        dz_dw = (0 * self.weights) + (1 * input_vector)\n        dL_db = (\n            dL_dp * dp_dz * dz_db\n        )\n        dL_dw = (\n            dL_dp * dp_dz * dz_dw\n        )\n        return dL_db, dL_dw\n\n    # function updating parameters through backpropogation\n    def _update_parameters(self, dL_db, dL_dw):\n        self.bias = self.bias - (dL_db * self.alpha)\n        self.weights = self.weights - (\n            dL_dw * self.alpha\n        )\n\n    # function training the neural network\n    def train(self, input_vectors, targets, iterations):\n        cumulative_errors = []\n        for current_iteration in range(iterations):\n            # Pick a data instance at random\n            random_data_index = np.random.randint(len(input_vectors))\n\n            input_vector = input_vectors[random_data_index]\n            target = targets[random_data_index]\n\n            # Compute the gradients and update the weights\n            dL_db, dL_dw = self._compute_gradients(\n                input_vector, target\n            )\n\n            self._update_parameters(dL_db, dL_dw)\n\n            # Measure the cumulative error for all the instances\n            if current_iteration % 100 == 0:\n                cumulative_error = 0\n                # Loop through all the instances to measure the error\n                for data_instance_index in range(len(input_vectors)):\n                    data_point = input_vectors[data_instance_index]\n                    target = targets[data_instance_index]\n\n                    prediction = self.predict(data_point)\n                    error = np.square(prediction - target)\n\n                    cumulative_error = cumulative_error + error\n                cumulative_errors.append(cumulative_error)\n\n        return cumulative_errors\n```\n:::\n\n\nWe consider the following data provided by the article on Real Python:\n\n: Table: Target and feature example data.\n\n| $x_1$ | $x_2$ | $y$ |\n|:-----:|:-----:|:---:|\n|   3   |  1.5  |  0  |\n|   2   |   1   |  1  |\n|   4   |  1.5  |  0  |\n|   3   |   4   |  1  |\n|  3.5  |  0.5  |  0  |\n|   2   |  0.5  |  1  |\n|  5.5  |   1   |  1  |\n|   1   |   1   |  0  |\n\n::: {#101deb80 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![Feature space grouped by target outputs.](index_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\nDefining the learning rate $\\alpha = 0.1$ and setting the seed for reproducibility we call an instance of the NN and produce a prediction using the first row: \n\n::: {#1afd87e6 .cell execution_count=4}\n``` {.python .cell-code}\nnp.random.seed(42)\nalpha = 0.1\ninput_vector = data[0,]\nneural_network = NeuralNetwork(alpha)\nneural_network.predict(input_vector)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nnp.float64(0.7784302101100966)\n```\n:::\n:::\n\n\nWe conclude by training our NN using the entire data set and 10000 iterations.  We produce a final plot showing the error for all training instances.\n\n::: {#852e281d .cell fig-width='8' execution_count=5}\n``` {.python .cell-code}\ntraining_error = neural_network.train(data, target, 10000)\nplt.plot(training_error)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Error for all training instances\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nThe result is reasonable but not very informative, the data set is small and random, therefore there is not a lot of information for the model to capture.  Furthermore, this is a poor measure of performance because it is using training data when in fact we are more interested in the models ability to correctly categorize new data.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}