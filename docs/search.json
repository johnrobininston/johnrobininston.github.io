[
  {
    "objectID": "topics.html",
    "href": "topics.html",
    "title": "Topics",
    "section": "",
    "text": "Data Science with R\n\n\n\nR\n\nData Science\n\n\n\n\n\n\n\nJohn Robin Inston\n\n\nFeb 12, 2026\n\n\n\n\n\n\n\n\n\n\n\nStochastic Processes\n\n\n\nStochastic Processes\n\n\n\n\n\n\n\nJohn Robin Inston\n\n\nDec 14, 2025\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "topics/data-science-R/index.html#information",
    "href": "topics/data-science-R/index.html#information",
    "title": "Data Science with R",
    "section": "‚úèÔ∏è Information",
    "text": "‚úèÔ∏è Information\nWelcome to Data Science with R where we outline a course of study for understanding how to use the programming language R for data science and statistical modelling.\nThe aim of this course is to provide a thorough introduction to programming in R for individuals who may have never programmed before. First, we outline how to install R on our operating system, how to download and use an Interactive Development Environment (IDE) such as RStudio, Positron or VSCode, and how to install packages on our system. We will then explore various ways to explore, manage and analyze data using both the in-built functionality in R as well as available libraries such as tidyverse."
  },
  {
    "objectID": "topics/data-science-R/index.html#notes",
    "href": "topics/data-science-R/index.html#notes",
    "title": "Data Science with R",
    "section": "‚úçÔ∏è Notes",
    "text": "‚úçÔ∏è Notes\nThis course is based fundamentally on the PSTAT10 Data Science Principles class I have taught in the past at UCSB. The course is split into the following topics:\n\nInstalling and using R and RStudio.\nOperators, Logic & Data Types.\nAtomic Data Structures.\nDataframes and Lists.\nFunctions.\nLooping and Branching.\nFundamentals of Probability Theory.\nSimulation and MC Methods.\nData Handling with the tidyverse package.\nPlotting with ggplot2.\nSQL Basics.\nSQL Aggregation and Joins."
  },
  {
    "objectID": "topics/data-science-R/index.html#materials",
    "href": "topics/data-science-R/index.html#materials",
    "title": "Data Science with R",
    "section": "üìö Materials",
    "text": "üìö Materials\nEach topic links to a website post with the relevant material. A pdf copy of the combined course notes can be downloaded here. Furthermore, each wesite post links to the corresponding youtube video going through the material.\nFor this course you will need to download the language R, your chosen IDE and Quarto using the following links:\n\nLink to download R & RStudio\nLink to download Quarto\n\nSome helpful resources and additional guides and linked below:\n\nDocumentation: Quarto for PDF documents\nDocumentation: Writing maths using LaTeX\nDocumentation: Plotting with ggplot\n\n\nPast Teaching Material\n\nPSTAT10 Summer 2025"
  },
  {
    "objectID": "topics/data-science-R/index.html#thanks",
    "href": "topics/data-science-R/index.html#thanks",
    "title": "Data Science with R",
    "section": "üôè Thanks",
    "text": "üôè Thanks\nIf you found any of this material helpful consider buying me a coffee but only if you can afford to! Thank you for visiting this course page. üòä"
  },
  {
    "objectID": "topics/stochastic-processes/index.html",
    "href": "topics/stochastic-processes/index.html",
    "title": "Stochastic Processes",
    "section": "",
    "text": "10 realizations of a symmetric random walk."
  },
  {
    "objectID": "topics/stochastic-processes/index.html#information",
    "href": "topics/stochastic-processes/index.html#information",
    "title": "Stochastic Processes",
    "section": "‚úèÔ∏è Information",
    "text": "‚úèÔ∏è Information\nIn this topic we study discrete time Stochastic Processes, countable sequences of random variables indexed in time. The material covered in these notes was prepareed during my time working as a teaching assistant under both Prof.¬†Tomoyuki Ichiba and Prof.¬†Vellaisamy Palaniappan for their respective versions of the PSTAT160A Stochastic Processes course at UC Santa Barbara."
  },
  {
    "objectID": "topics/stochastic-processes/index.html#notes",
    "href": "topics/stochastic-processes/index.html#notes",
    "title": "Stochastic Processes",
    "section": "‚úçÔ∏è Notes",
    "text": "‚úçÔ∏è Notes\n\nProbability Theory\nGenerating Functions\nMarkov Chains\nRandom Walks\nBranching Processes\nMarkov Chain Monte Carlo (MCMC) Methods\nPoisson Processes"
  },
  {
    "objectID": "topics/stochastic-processes/index.html#materials",
    "href": "topics/stochastic-processes/index.html#materials",
    "title": "Stochastic Processes",
    "section": "üìö Materials",
    "text": "üìö Materials\nBelow are links to documents I have prepared for my past sections as well as a collection of additional exercise sheets covering similar questions to those covered in past assignments and exams. These documents can also be found in the respective course pages on my GitHub.\n\nPast Teaching Material\n\nPSTAT160A Fall 2025 (Prof.¬†Tomoyuki Ichiba)\nPSTAT160A Winter 2026 (Prof.¬†Vellaisamy Palaniappan)"
  },
  {
    "objectID": "topics/stochastic-processes/index.html#thanks",
    "href": "topics/stochastic-processes/index.html#thanks",
    "title": "Stochastic Processes",
    "section": "üôè Thanks!",
    "text": "üôè Thanks!\nIf you found any of this material helpful consider buying me a coffee but only if you can afford to! Thank you for visiting this course page. üòä"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Markov Chains\n\n\nMarkov property, transition probabilities, state distributions, chain classification, Chapman-Kolmogorov equations and limiting distributions.\n\n\n\nStochastic Processes\n\n\n\n\n\n\n\n\n\nFeb 12, 2026\n\n\nJohn Robin Inston\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Steps\n\n\n\n\n\n\nBlog\n\n\n\n\n\n\n\n\n\nJan 28, 2026\n\n\nJohn Robin Inston\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/markov-chains/index.html",
    "href": "posts/markov-chains/index.html",
    "title": "Markov Chains",
    "section": "",
    "text": "In probability theory a stochastic process is said to satisfy the Markov property if its future evolution is independent of its history, that is, the process is memoryless. Consider a discrete time stochastic process \\(\\{X_t\\}_{t\\in\\mathbb{N}}\\) defined in the probability space \\((\\Omega,\\mathcal{F}, \\mathbb{P})\\).\n\nDefinition: (Markov Property)\nThe discrete time stochastic process \\(\\{X_t\\}_{t\\in\\mathbb{N}}\\) is said to satisfy the Markov property if \\[\\mathbb{P}(X_{n+1}=x_{n+1}|X_n=x_n, ..., X_1=x_1)=\\mathbb{P}(X_{n+1}=x_{n+1}|X_n=x_n).\\]\n\nIntuitively, we see that we gain no additional information about the probability of the next value of the process by conditioning on the entire history rather than the latest value. Any process satisfying the markov property is called a Markov process.\n\n\n\nA Markov process with a discrete state space is known as a Markov chain (MC). Assume that \\(\\{X_t\\}_{t\\in\\mathbb{N}}\\) is a Markov chain with finite state space \\(\\mathcal{X}:=\\{1, ..., M\\}\\). The evolution of the chain in time is described by transition probabilities \\(p_{i,j}^{(n)}(t)\\) which give the probability that the chain is in state \\(j\\) at time \\(t+n\\) given that the chain was in \\(i\\) at time \\(t\\) \\[\np_{i,j}^{(n)}(t)=\\mathbb{P}(X_{t+n}=j|X_{t}=i).\n\\]\nWe summarize the transition probabilities in an \\(M\\times M\\) transition probability matrix denoted by \\[\nP^{(n)}(t)=(p_{i,j}^{(n)}(t))_{i,j\\in\\mathcal{X}}.\n\\]\nThe transition probability matrix is stochastic which means that for all \\(n,t\\in\\mathbb{N}\\) we have:\n\n\\(0\\leq p_{i,j}^{(n)}(t)&lt;1\\) for all \\(i,j\\in \\mathcal{X}\\),\n\\(\\sum_{{j\\in S}}p_{{i,j}}^{(n)}(t)=1\\) for all \\(i\\in \\mathcal{X}\\) (the rows sum to 1).\n\nTo simplify notation we denote the one-step transition probablity matrix\n\\[\nP(t)=(p_{i,j}(t))_{i,j\\in\\mathcal{X}}.\n\\]\nThe transition probabilities are stationary when they are invariant in time, that is \\[\np_{i,j}^{(n)}(t)=p_{i,j}^{(n)}(s);~~\\forall s,t\\in\\mathbb{N}.\n\\]\nWe say that a MC is homogenous if it has stationary transition probabilities. For homogenous MCs we are often interested in the one-step transition probabilities and so again to clarify notation we denote the one-step transition probability matrix for homogenous MCs as \\[\nP=(p_{i,j})_{i,j\\in\\mathcal{X}}.\n\\]\nExample: Consider a homogenous MC \\(\\{X_t\\}\\) with finite state space \\(\\mathcal{S}:=\\{1,2,3\\}\\) with transition probability matrix \\[\nP = \\begin{bmatrix}\n0 & 0.6 & 0.2 & 0.2 \\\\\n0.75 & 0 & 0.25 & 0 \\\\\n0.25 & 0.25 & 0.25 & 0.25 \\\\\n0.25 & 0 & 0.25 & 0.5\n\\end{bmatrix}.\n\\]\n\n\n\nTo gain deeper insight into the behavior of the chain we typically will produce transition probability diagram which takes the form of a directed graph with nodes denoting states and weighted directed arrows the transition probabilities. The transition probability diagram for our example is\n\n\n\nTransition probability diagram.\n\n\nThere are several alternate versions of this diagram including the integer weighted diagram. To construct the integer-weighted diagram we compute the inter-weighted matrix by multiplying elements of each row by the lowest common multiple for that row. The integer weighted matrix for our example is \\[\nP = \\begin{bmatrix}\n0 & 3 & 1 & 1 \\\\\n3 & 0 & 1 & 0 \\\\\n1 & 1 & 1 & 1 \\\\\n1 & 0 & 1 & 2\n\\end{bmatrix}.\n\\]\nThe corresponding diagram is\n\n\n\nInteger-weighted transition probability diagram.\n\n\nSince our diagram has a symmetric integer-weighted transition matrix we can also write the non-directed version by combining the equally weighted directed arrows into a single non-directed line\n\n\n\nInteger-weighted non-directed ransition probability diagram.\n\n\n\n\n\nThe state distribution \\(\\mu_i(t)\\) gives the probability that the chain is in state \\(i\\) at time \\(t\\) \\[\n\\mu_i(t)=\\mathbb{P}(X_{t}=i).\n\\]\nFor convenience we summarize the state distributions in vector notation \\[\n\\boldsymbol{\\mu}(t)=(\\mu_i(t))_{i \\in \\mathcal{X}}.\n\\]\nOf particular interest is the initial state distribution \\[\n\\boldsymbol{\\mu}(0)=\\boldsymbol{\\mu}=(\\mu_i(0))_{i \\in\\mathcal{X}}.\n\\]\nFor conveinence we also introduce the following notation for process vectors \\[\nX_{1:t} = (X_1, ..., X_t),\n\\]\nwhere similarly \\(X_{1:t}=x_{1:t}\\) is equivalent to \\(X_1=x_1, ..., X_t=x_t\\).\nFinite-dimensional distributions allow us to evaluate the behavior of the distribution of infinite length processes by selecting some sub-sequence of index values to evaluate their joint distribution.\n\nProposition (Finite-Dimensional Distributions):\nThe finite-dimensional distributions of \\(X=(X_{n})_{n}\\) are determined by the initial mass \\(\\mu^{(0)}\\) and transition matrix \\(P\\) where \\[P(X_{0}=i_{0}, \\dots, X_{n}=i_{n})=\\mu_{{x_{0}}}^{(0)}p_{x_{0}, x_{1}}(0)p_{x_{1}, x_{2}}(1)\\cdots p_{x_{n-1}, x_{n}}(n-1).\\]\n\nProof:\nFrom application of the Chain Rule and the Markov Property we see that \\[\n\\begin{align}  \n\\mathbb{P}&(X_{1 : n}=x_{1: n}) & \\\\\n& = \\mathbb{P}(X_{0}=x_{0}) \\mathbb{P}(X_{1}=x_{1}|X_{0}=x_{0})\\mathbb{P}(X_{2}=x_{2}|X_{0 : 1}=x_{0 : 1})\\cdots \\\\\n& \\quad \\cdots \\mathbb{P}(X_{n}=x_{n}|X_{0 : n-1}=x_{0: n-1})  \\\\\n  & =\\mathbb{P}(X_{0}=x_{0})\\mathbb{P}(X_{1}=x_{1}|X_{0}=x_{0})\\mathbb{P}(X_{2}=x_{2}|X_{1}=x_{1})\\cdots \\\\\n  & \\quad \\cdots \\mathbb{P}(X_{n}=x_{n}|X_{n-1}=x_{n-1})  \\\\\n& = \\mu_{{x_{0}}}^{(0)}p_{x_{0}, x_{1}}(0)p_{x_{1}, x_{2}}(1)\\cdots p_{x_{n-1}, x_{n}}(n-1),\n\\end{align}\n\\]\nas required.\n\\(\\square\\)\n\nProposition: (Multi-step Transition Probabilities for homogenous MCs)\nLet \\(\\{ X_{n} \\}\\) be a homogenous MC with one-step transition probability matrix \\(P\\). The \\(m\\)-step transition probabilities are given by raising the one-step transition probability matrix to the \\(m\\)-th power \\[P^{(m)}=(\\mathbb{P}(X_{m}=j|X_{0}=i))_{i,j}=P^m.\\]\n\nProof:\nThe result follows immediately from the Markov property since \\[\n\\begin{aligned}\n\\mathbb{P}(X_{m}|X_{0}) & =\\mathbb{P}(X_{m}|X_{m-1})\\mathbb{P}(X_{m-1}|X_{0}) \\\\\n& = \\qquad \\vdots \\\\\n& = \\mathbb{P}(X_{m}|X_{m-1})\\cdots \\mathbb{P}(X_{1}|X_{0}) \\\\\n& = [\\mathbb{P}(X_{1}|X_{0})]^m.\n\\end{aligned}\n\\]\n\\(\\square\\)\nExample 1: Consider a homogenous MC with state space \\(\\mathcal{S}:=\\{ 1,2,3 \\}\\) with transition probability matrix \\(P\\) and initial state distribution \\(\\mu\\) given respectively by \\[\nP:=\\begin{bmatrix}\n0.4 & 0.6 & 0 \\\\\n0.7 & 0.3 & 0  \\\\\n0 & 0.1 & 0.9\n\\end{bmatrix},~~\\mu=\\begin{bmatrix}\n0.1 & 0.2 & 0.7\n\\end{bmatrix}.\n\\]\nThe transition probability diagram for this chain is\n\n\n\nTransition Probability Diagram.\n\n\nSuppose we wish to determine the \\(3\\)-step transition probabilities. Applying Proposition 1 we can compute \\(\\mathbb{P}(X_3|X_0)\\) as \\(P^3\\):\n\nP = np.array([\n    [.4, .6, 0],\n    [.7, .3, 0],\n    [0, .1, .9]\n])\nP3 = np.linalg.matrix_power(P,3); print(P3)\n\n[[0.526 0.474 0.   ]\n [0.553 0.447 0.   ]\n [0.112 0.159 0.729]]\n\n\nFurthermore, let us suppose that we wish to determine the 3-step state distribution. From the finite-dimensional distribution we see that \\[\n\\begin{aligned}\n\\mathbb{P}(X_3) & =\\mathbb{P}(X_0)\\mathbb{P}(X_1|X_0)\\mathbb{P}(X_2|X_1)\\mathbb{P}(X_3|X_2) \\\\\n& = \\boldsymbol{\\mu}P^3.\n\\end{aligned}\n\\]\nThus we can compute:\n\nmu = np.array([\n    0.1, 0.2, 0.7\n])\nmu@(P3)\n\narray([0.2416, 0.2481, 0.5103])"
  },
  {
    "objectID": "posts/markov-chains/index.html#markov-chains",
    "href": "posts/markov-chains/index.html#markov-chains",
    "title": "Markov Chains",
    "section": "",
    "text": "In probability theory a stochastic process is said to satisfy the Markov property if its future evolution is independent of its history, that is, the process is memoryless. Consider a discrete time stochastic process \\(\\{X_t\\}_{t\\in\\mathbb{N}}\\) defined in the probability space \\((\\Omega,\\mathcal{F}, \\mathbb{P})\\).\n\nDefinition: (Markov Property)\nThe discrete time stochastic process \\(\\{X_t\\}_{t\\in\\mathbb{N}}\\) is said to satisfy the Markov property if \\[\\mathbb{P}(X_{n+1}=x_{n+1}|X_n=x_n, ..., X_1=x_1)=\\mathbb{P}(X_{n+1}=x_{n+1}|X_n=x_n).\\]\n\nIntuitively, we see that we gain no additional information about the probability of the next value of the process by conditioning on the entire history rather than the latest value. Any process satisfying the markov property is called a Markov process.\n\n\n\nA Markov process with a discrete state space is known as a Markov chain (MC). Assume that \\(\\{X_t\\}_{t\\in\\mathbb{N}}\\) is a Markov chain with finite state space \\(\\mathcal{X}:=\\{1, ..., M\\}\\). The evolution of the chain in time is described by transition probabilities \\(p_{i,j}^{(n)}(t)\\) which give the probability that the chain is in state \\(j\\) at time \\(t+n\\) given that the chain was in \\(i\\) at time \\(t\\) \\[\np_{i,j}^{(n)}(t)=\\mathbb{P}(X_{t+n}=j|X_{t}=i).\n\\]\nWe summarize the transition probabilities in an \\(M\\times M\\) transition probability matrix denoted by \\[\nP^{(n)}(t)=(p_{i,j}^{(n)}(t))_{i,j\\in\\mathcal{X}}.\n\\]\nThe transition probability matrix is stochastic which means that for all \\(n,t\\in\\mathbb{N}\\) we have:\n\n\\(0\\leq p_{i,j}^{(n)}(t)&lt;1\\) for all \\(i,j\\in \\mathcal{X}\\),\n\\(\\sum_{{j\\in S}}p_{{i,j}}^{(n)}(t)=1\\) for all \\(i\\in \\mathcal{X}\\) (the rows sum to 1).\n\nTo simplify notation we denote the one-step transition probablity matrix\n\\[\nP(t)=(p_{i,j}(t))_{i,j\\in\\mathcal{X}}.\n\\]\nThe transition probabilities are stationary when they are invariant in time, that is \\[\np_{i,j}^{(n)}(t)=p_{i,j}^{(n)}(s);~~\\forall s,t\\in\\mathbb{N}.\n\\]\nWe say that a MC is homogenous if it has stationary transition probabilities. For homogenous MCs we are often interested in the one-step transition probabilities and so again to clarify notation we denote the one-step transition probability matrix for homogenous MCs as \\[\nP=(p_{i,j})_{i,j\\in\\mathcal{X}}.\n\\]\nExample: Consider a homogenous MC \\(\\{X_t\\}\\) with finite state space \\(\\mathcal{S}:=\\{1,2,3\\}\\) with transition probability matrix \\[\nP = \\begin{bmatrix}\n0 & 0.6 & 0.2 & 0.2 \\\\\n0.75 & 0 & 0.25 & 0 \\\\\n0.25 & 0.25 & 0.25 & 0.25 \\\\\n0.25 & 0 & 0.25 & 0.5\n\\end{bmatrix}.\n\\]\n\n\n\nTo gain deeper insight into the behavior of the chain we typically will produce transition probability diagram which takes the form of a directed graph with nodes denoting states and weighted directed arrows the transition probabilities. The transition probability diagram for our example is\n\n\n\nTransition probability diagram.\n\n\nThere are several alternate versions of this diagram including the integer weighted diagram. To construct the integer-weighted diagram we compute the inter-weighted matrix by multiplying elements of each row by the lowest common multiple for that row. The integer weighted matrix for our example is \\[\nP = \\begin{bmatrix}\n0 & 3 & 1 & 1 \\\\\n3 & 0 & 1 & 0 \\\\\n1 & 1 & 1 & 1 \\\\\n1 & 0 & 1 & 2\n\\end{bmatrix}.\n\\]\nThe corresponding diagram is\n\n\n\nInteger-weighted transition probability diagram.\n\n\nSince our diagram has a symmetric integer-weighted transition matrix we can also write the non-directed version by combining the equally weighted directed arrows into a single non-directed line\n\n\n\nInteger-weighted non-directed ransition probability diagram.\n\n\n\n\n\nThe state distribution \\(\\mu_i(t)\\) gives the probability that the chain is in state \\(i\\) at time \\(t\\) \\[\n\\mu_i(t)=\\mathbb{P}(X_{t}=i).\n\\]\nFor convenience we summarize the state distributions in vector notation \\[\n\\boldsymbol{\\mu}(t)=(\\mu_i(t))_{i \\in \\mathcal{X}}.\n\\]\nOf particular interest is the initial state distribution \\[\n\\boldsymbol{\\mu}(0)=\\boldsymbol{\\mu}=(\\mu_i(0))_{i \\in\\mathcal{X}}.\n\\]\nFor conveinence we also introduce the following notation for process vectors \\[\nX_{1:t} = (X_1, ..., X_t),\n\\]\nwhere similarly \\(X_{1:t}=x_{1:t}\\) is equivalent to \\(X_1=x_1, ..., X_t=x_t\\).\nFinite-dimensional distributions allow us to evaluate the behavior of the distribution of infinite length processes by selecting some sub-sequence of index values to evaluate their joint distribution.\n\nProposition (Finite-Dimensional Distributions):\nThe finite-dimensional distributions of \\(X=(X_{n})_{n}\\) are determined by the initial mass \\(\\mu^{(0)}\\) and transition matrix \\(P\\) where \\[P(X_{0}=i_{0}, \\dots, X_{n}=i_{n})=\\mu_{{x_{0}}}^{(0)}p_{x_{0}, x_{1}}(0)p_{x_{1}, x_{2}}(1)\\cdots p_{x_{n-1}, x_{n}}(n-1).\\]\n\nProof:\nFrom application of the Chain Rule and the Markov Property we see that \\[\n\\begin{align}  \n\\mathbb{P}&(X_{1 : n}=x_{1: n}) & \\\\\n& = \\mathbb{P}(X_{0}=x_{0}) \\mathbb{P}(X_{1}=x_{1}|X_{0}=x_{0})\\mathbb{P}(X_{2}=x_{2}|X_{0 : 1}=x_{0 : 1})\\cdots \\\\\n& \\quad \\cdots \\mathbb{P}(X_{n}=x_{n}|X_{0 : n-1}=x_{0: n-1})  \\\\\n  & =\\mathbb{P}(X_{0}=x_{0})\\mathbb{P}(X_{1}=x_{1}|X_{0}=x_{0})\\mathbb{P}(X_{2}=x_{2}|X_{1}=x_{1})\\cdots \\\\\n  & \\quad \\cdots \\mathbb{P}(X_{n}=x_{n}|X_{n-1}=x_{n-1})  \\\\\n& = \\mu_{{x_{0}}}^{(0)}p_{x_{0}, x_{1}}(0)p_{x_{1}, x_{2}}(1)\\cdots p_{x_{n-1}, x_{n}}(n-1),\n\\end{align}\n\\]\nas required.\n\\(\\square\\)\n\nProposition: (Multi-step Transition Probabilities for homogenous MCs)\nLet \\(\\{ X_{n} \\}\\) be a homogenous MC with one-step transition probability matrix \\(P\\). The \\(m\\)-step transition probabilities are given by raising the one-step transition probability matrix to the \\(m\\)-th power \\[P^{(m)}=(\\mathbb{P}(X_{m}=j|X_{0}=i))_{i,j}=P^m.\\]\n\nProof:\nThe result follows immediately from the Markov property since \\[\n\\begin{aligned}\n\\mathbb{P}(X_{m}|X_{0}) & =\\mathbb{P}(X_{m}|X_{m-1})\\mathbb{P}(X_{m-1}|X_{0}) \\\\\n& = \\qquad \\vdots \\\\\n& = \\mathbb{P}(X_{m}|X_{m-1})\\cdots \\mathbb{P}(X_{1}|X_{0}) \\\\\n& = [\\mathbb{P}(X_{1}|X_{0})]^m.\n\\end{aligned}\n\\]\n\\(\\square\\)\nExample 1: Consider a homogenous MC with state space \\(\\mathcal{S}:=\\{ 1,2,3 \\}\\) with transition probability matrix \\(P\\) and initial state distribution \\(\\mu\\) given respectively by \\[\nP:=\\begin{bmatrix}\n0.4 & 0.6 & 0 \\\\\n0.7 & 0.3 & 0  \\\\\n0 & 0.1 & 0.9\n\\end{bmatrix},~~\\mu=\\begin{bmatrix}\n0.1 & 0.2 & 0.7\n\\end{bmatrix}.\n\\]\nThe transition probability diagram for this chain is\n\n\n\nTransition Probability Diagram.\n\n\nSuppose we wish to determine the \\(3\\)-step transition probabilities. Applying Proposition 1 we can compute \\(\\mathbb{P}(X_3|X_0)\\) as \\(P^3\\):\n\nP = np.array([\n    [.4, .6, 0],\n    [.7, .3, 0],\n    [0, .1, .9]\n])\nP3 = np.linalg.matrix_power(P,3); print(P3)\n\n[[0.526 0.474 0.   ]\n [0.553 0.447 0.   ]\n [0.112 0.159 0.729]]\n\n\nFurthermore, let us suppose that we wish to determine the 3-step state distribution. From the finite-dimensional distribution we see that \\[\n\\begin{aligned}\n\\mathbb{P}(X_3) & =\\mathbb{P}(X_0)\\mathbb{P}(X_1|X_0)\\mathbb{P}(X_2|X_1)\\mathbb{P}(X_3|X_2) \\\\\n& = \\boldsymbol{\\mu}P^3.\n\\end{aligned}\n\\]\nThus we can compute:\n\nmu = np.array([\n    0.1, 0.2, 0.7\n])\nmu@(P3)\n\narray([0.2416, 0.2481, 0.5103])"
  },
  {
    "objectID": "posts/markov-chains/index.html#mc-classification",
    "href": "posts/markov-chains/index.html#mc-classification",
    "title": "Markov Chains",
    "section": "MC Classification",
    "text": "MC Classification\n\nState Classification\nWe can classify MC states based on how they appear in the chain and their behavior in the limit. States can be persistent (recurrent), null-persistent, transient, absorbing, periodic and ergodic.\n\nDefinition: (Persistent States) A state \\(j\\) is persistent if the probability that the process will return to \\(j\\) given that it started at \\(j\\) eventually is 1, that is \\[\\mathbb{P}(X_{n}=j~\\text{for some}~n\\geq 1|X_{0}=j)=1.\\]\n\nA sub-class of persistent states are null-persistent states which are persistent thats that have an infinite mean recurrence time. States that are not null-persistent are said to be positive_persistent.\nFor example, consider a symmetric random walk. Any point in the state space is null-persistent since the random walk can always return but may take infinitely long to do so. We discuss null-persistent states in a later note on MCs.\n\nDefinition: (Transient States)\nAlternatively, if a state \\(j\\) is not persistent it must be transient, that is, the probability that the process will return to \\(j\\) given that it started at \\(j\\) eventually is 0 i.e.¬†the processes structure prevents it from returning. This is equivalent to writing \\[\\mathbb{P}(X_{n}=i~\\text{for some}~n\\geq 1|X_{0}=i)&lt;1.\\]\n\n\nDefinition: (Periodic States)\nThe period of a state \\(j\\) is the greatest common divider of all \\(n\\) for which \\(p_{{i,i}}(n)&gt;0\\) i.e.¬† \\[d(i)=gcd\\{n:p_{i,i}(n)&gt;0\\}.\\] If \\(d(i)=1\\) then the state \\(j\\) is aperiodic and otherwise, the state is said to be periodic.\n\n\nDefinition: (Ergodic)\nA state \\(j\\) is ergodic if it is positive persistent and aperiodic i.e.¬†\\(\\mu_{i,i}&lt;\\infty\\) and \\(d(i)=1\\).\n\n\nDefinition: (Absorbing)\nA state \\(j\\) is absorbing if \\(p_{j,j}=1\\), that is the probability of leaving state \\(j\\) once the process has entered is 0.\n\n\n\nChain Classification\nTo classify chains we first define state communication, that is when paths between states have non-zero probability.\n\nDefinition: (Communicating States)\nFor MC \\(\\{ X_{t} \\}\\) state \\(i\\) communicates with state \\(j\\), denoted \\(i \\to j\\), if \\(p_{i,j}(n)&gt;0\\) for some \\(n\\). If \\(i\\to j\\) and \\(j \\to i\\) we say that \\(i\\) and \\(j\\) intercommunicate, denoted \\(i \\leftrightarrow j\\).\n\nWe can show that intercommunication is an equivalence class and thus can be used to split MC state spaces into communication classes. For \\(i,j\\) in the same communication class we have:\n\nStates \\(i\\) and \\(j\\) have the same period;\nState \\(i\\) is transient iff \\(j\\) is transient; and\nState \\(i\\) is null persistent iff \\(j\\) is.\n\n\nDefinition: (Irreducibility)\nA set of states \\(C\\) is irreducible if for all \\(i,j\\in C\\), \\(i\\leftrightarrow j\\), that is all states within \\(C\\) inter-communicate.\n\n\nDefinition: (Closed)\nA set of states \\(C\\) is closed if for all \\(i \\in C\\), \\(p_{i,j}=0\\) for all \\(j\\not\\in C\\), that is, the chain never leaves \\(C\\) once it has entered. Not that clearly the set consisting of one absorbing state is closed.\n\n\nExample: Consider the MC transition diagram shown in the figure below.\n\n\n\nExample transition probability diagram.\n\n\nWe define \\(S_1:=\\{5\\}\\), \\(S_2:=\\{1,2,3,4\\}\\) and \\(S_3:=\\{6,7\\}\\). The chain is clearly reducible since \\(2\\to 5\\) and \\(3\\to 6\\) are one way journies. We can see that \\(5\\) is absorbing since the chain can never leave once it arrives. We see that states \\(1,2,3,4\\) are transient since the chain will visit them and never return at some point, and states \\(6\\) and \\(7\\) are recurrent since the chain will always revisit them. We say that \\(S_1\\) is an closed recurrent (absorbing) class, \\(S_2\\) is a transient class and \\(S_3\\) is a closed recurrent class."
  },
  {
    "objectID": "posts/markov-chains/index.html#chapman-kolmogorov-equations",
    "href": "posts/markov-chains/index.html#chapman-kolmogorov-equations",
    "title": "Markov Chains",
    "section": "Chapman Kolmogorov Equations",
    "text": "Chapman Kolmogorov Equations\nThe Chapman-Kolmogorov Equations (CKE) relate the joint probability distributions of different sets of coordinates on a stochastic process.\n\nTheorem 2: (Chapman-Kolmogorov Equations)\nFor a discrete time countable state homogeneous Markov chain the Chapman-Kolmogorov equations state that \\[P_{n+m}=P_{n}\\cdot P_{m},\\] or equivalently \\[p^{(n+m)}_{i,j}=\\sum_{k\\in S}p^{(n)}_{i,k}p^{(m)}_{k,j}.\\]\n\nA similar result holds for distributions \\[\n\\mu_{j}^{(n+m)}=\\sum_{i \\in S}\\mu_{i}^{(n)}p_{i,j}(m),\n\\]\nor in matrix form \\[\n\\boldsymbol{\\mu}^{(n+m)}=\\boldsymbol{\\mu}^{(n)}P_{m}=\\boldsymbol{\\mu}^{(n)}P^m\\quad\\&\\quad \\boldsymbol{\\mu}^{(n)}=\\boldsymbol{\\mu}^{(0)}P^n.\n\\]"
  },
  {
    "objectID": "posts/markov-chains/index.html#limiting-distributions",
    "href": "posts/markov-chains/index.html#limiting-distributions",
    "title": "Markov Chains",
    "section": "Limiting Distributions",
    "text": "Limiting Distributions\n\nLimiting Distribution\nConsider the long-term behavior of a MC \\(\\{ X_{n} \\}_{n=0}^\\infty\\) when \\(n \\to \\infty\\). It is possible for the chain to converge to a particular state (e.g.¬†a Galton-Watson-Bienaym√© (GWB) Branching Process can converge to 0). Additionally, it is possible for a MC to converge to some random variable \\(X~a.s.\\) as \\(n\\to \\infty\\). Intuitively, if a Markov chain runs for a long time it generally doesn‚Äôt converge because it is always jumping around but its distribution can settle down.\n\nDefinition: (Limiting Distribution) The limiting distribution of a homogenous MC with initial distribution \\(\\boldsymbol{\\mu}\\) and TP matrix \\(P\\) is the distribution vector \\[\\lambda:=\\lim_{n\\to\\infty}\\mu\\cdot P^n.\\]\n\nThis is often a challenging limit to find analytically and even numerically often requires Monte-Carlo simulations.\n Example: Reconsider our last MC example with transition diagram given in the figure below.\n\n\n\nComplicated transition diagram.\n\n\nWe know that the \\(S_2:=\\{1,2,3,4\\}\\) is transient and so given enough time to run, the chain will either end on in \\(S_1=\\{5\\}\\) or \\(S_3=\\{6,7\\}\\). We therefore need to ask the following questions:\n\nHow do we know what proportion of the time the chain ends in each class.\nIf the chain ends up in closed irreducible \\(S_3\\), what is the behavior of the state distribution in the limit? \\(\\rightarrow\\) Stationary Distributions\n\n\n\n\nStationary Distribution\nA process is strictly stationary if its distribution does not change under translations, i.e.¬†over time. More formally we give the following definition.\n\nDefinition: (Strictly Stationary Process)\nA process \\(\\{ X_{n},~n\\geq 0 \\}\\) is strictly stationary if for any integers \\(m\\geq 0\\) and \\(k&gt;0\\), we have \\[(Y_{0}, Y_{1}, \\dots Y_{m})\\stackrel{\\mathcal{D}}{=}(Y_{k}, Y_{k+1}, \\dots, Y_{k+m})\\] that is, the distribution does not change under translations.\n\nThis is often a challenging condition to show and so we also define a weak stationarity of the mean and covariance being invariant to changes in time. See oour discussion of Time Series Anaysis for more detailed noted on stationarity.\n\nDefinition: (Stationary Distribution)\nThe vector \\(\\pi=(\\pi_{j},~j \\in S)\\) is called a stationary distribution of a Markov chain if:\n\n\\(\\pi_{j}\\geq 0\\).\n\\(\\sum_{j\\in S}\\pi_{j}=1\\).\n\\(\\pi=\\pi P\\).\n\n\nNote that (3) can equivalently be written as \\[\n\\pi_{j}=\\sum_{i \\in S}\\pi_{i}p_{i,j}\\text{ for all }j \\in S.\n\\]\nAlso note that \\(\\pi P^2=\\pi P\\cdot P=\\pi P=\\pi\\) and similarly, for all \\(n&gt;1\\), \\(\\pi P^n=\\pi\\), that is \\(\\pi_{j}=\\sum_{i\\in S}\\pi_{i}p_{i,j}(n)\\) for all \\(j \\in S\\).\n\nTheorem: (Stationary Distribution) For any irreducible MC \\(\\{X\\}\\) with a finite state space \\(\\mathcal{X}\\) there exists a stationary distribiton \\(\\pi\\).\n\nProof:\nLet \\(P\\) be the transition matrix on a finite state space \\(\\mathcal{X}\\), and assume the chain is irreducible. In a finite Markov chain, at least one state is recurrent (otherwise all states would be transient, which is impossible in a finite state space because probability mass would have to ‚Äúescape‚Äù forever without accumulating anywhere). Since the chain is irreducible, all states communicate with that recurrent state, hence every state is recurrent.\nBecause the chain is finite and irreducible, starting from any state \\(i\\), every other state \\(j\\) is hit with positive probability in at most \\(|S|-1\\) steps along some path. In particular, there exists an \\(m\\ge 1\\) with \\[\np^{(m)}_{ii} &gt; 0.\n\\]\nLet \\(\\tau_i^+ = \\inf\\{n\\ge1: X_n=i\\}\\) be the first return time to \\(i\\). Then each block of \\(m\\) steps has probability at least \\(p^{(m)}_{ii}\\) of containing a return to \\(i\\) at its endpoint, so \\[\n\\mathbb{P}_i(\\tau_i^+ &gt; km) \\le (1-p^{(m)}_{ii})^k.\n\\]\nHence the tail of \\(\\tau_i^+\\) is geometrically bounded, implying \\[\n\\mathbb{E}_i[\\tau_i^+] &lt; \\infty.\n\\]\nSo the chain is positive recurrent. Define, for each \\(j\\in S\\), \\[\n\\pi_j := \\frac{\\mathbb{E}_i\\!\\left[\\sum_{n=0}^{\\tau_i^+-1}\\mathbf{1}\\{X_n=j\\}\\right]}{\\mathbb{E}_i[\\tau_i^+]}.\n\\]\nThis is the expected fraction of time spent in state \\(j\\) during an \\(i\\to i\\) regeneration cycle. Clearly \\(\\pi_j\\ge 0\\) and \\[\n\\sum_{j\\in S}\\pi_j\n= \\frac{\\mathbb{E}_i\\!\\left[\\sum_{n=0}^{\\tau_i^+-1}\\sum_{j\\in S}\\mathbf{1}\\{X_n=j\\}\\right]}{\\mathbb{E}_i[\\tau_i^+]}\n= \\frac{\\mathbb{E}_i[\\tau_i^+]}{\\mathbb{E}_i[\\tau_i^+]}=1,\n\\]\nso \\(\\pi\\) is a probability distribution. To show \\(\\pi P=\\pi\\), note that the expected number of visits to \\(j\\) during a cycle equals the expected number of transitions into \\(j\\) during the cycle (up to the regeneration boundary, which contributes no net imbalance because the cycle starts and ends at \\(i\\)). Formally, by the Markov property and counting transitions within the cycle, \\[\n\\mathbb{E}_i\\!\\left[\\sum_{n=0}^{\\tau_i^+-1}\\mathbf{1}\\{X_{n+1}=j\\}\\right]\n= \\sum_{k\\in S}\\mathbb{E}_i\\!\\left[\\sum_{n=0}^{\\tau_i^+-1}\\mathbf{1}\\{X_n=k\\}\\right]p_{kj}.\n\\]\nDivide both sides by \\(\\mathbb{E}_i[\\tau_i^+]\\) to get \\[\n\\pi_j = \\sum_{k\\in S}\\pi_k p_{kj},\n\\]\nwhich is exactly \\(\\pi P=\\pi\\). Thus a stationary distribution exists.\n\\(\\square\\)\nExample: Consider a Markov chain describing the meals served by a restaurant with transition graph shown below in Figure 1 provided by helpful video on Markov chains by Normalized Nerd.\n\n\n\nExample transition diagram for a MC describing meals served at a restaurant.\n\n\nTaking state 1 to be hamburger, state 2 to be pizza and state 3 to be hotdog, the transition matrix \\(P\\) can be written as \\[\nP=\\begin{bmatrix}\n0.2 & 0.6 & 0.2  \\\\\n0.3 & 0 & 0.7  \\\\\n0.5 & 0 & 0.5\n\\end{bmatrix}.\n\\]\nGiven the restaurant first serves pizza we can define the initial distribution \\(\\pi_{0}=\\begin{bmatrix}0 & 1 & 0\\end{bmatrix}\\). Applying the transition matrix \\(P\\) we get \\[\n\\pi_{0}P=\\begin{bmatrix}\n0 & 1 & 0\n\\end{bmatrix}\\cdot \\begin{bmatrix}\n0.2 & 0.6 & 0.2  \\\\\n0.3 & 0 & 0.7  \\\\\n0.5 & 0 & 0.5\n\\end{bmatrix}=\\begin{bmatrix}\n0.3 & 0 & 0.7\n\\end{bmatrix}=\\pi_{1},\n\\]\nthe second state future transition probabilities. Repeating this step for \\(\\pi_{1}\\) we have \\[\n\\pi_{0}P=\\begin{bmatrix}\n0.3 & 0 & 0.7\n\\end{bmatrix}\\cdot \\begin{bmatrix}\n0.2 & 0.6 & 0.2  \\\\\n0.3 & 0 & 0.7  \\\\\n0.5 & 0 & 0.5\n\\end{bmatrix}=\\begin{bmatrix}\n0.41 & 0.18 & 0.41\n\\end{bmatrix}=\\pi_{2},\n\\]\nIf a stationary distribution \\(\\pi\\) exists it would mean that as \\(\\pi_{0}, \\pi_{1}, \\dots\\) continues, eventually it will reach a point where it doesn‚Äôt change when \\(P\\) is applied, hence using linear algebra we can write the expression \\[\n\\pi P=\\pi,\n\\]\nsee Definition 2 above. Additionally, since \\(\\pi\\) is a vector of probabilities we have that \\(\\pi(1)+\\pi(2)+\\pi(3)=1\\) and solving this system gives the stationary distribution \\[\n\\pi=\\begin{bmatrix}\n\\frac{25}{71} & \\frac{15}{71} & \\frac{31}{71}\n\\end{bmatrix}.\n\\]\nWe can numerically confirm our result by iteratively applying the transition probability matrix: \n\nP = np.array([\n    [.2, .6, .2],\n    [.3, 0, .7],\n    [.5, 0, .5]\n])\npi = np.array([\n    0, 1, 0\n])\npi_list = []\npi_list.append(pi)\n\nn = 50\n\nfor _ in range(n):\n    pi = pi @ P\n    pi_list.append(pi) \n\npi_list = np.array(pi_list).squeeze()\nfig, ax = plt.subplots()\nax.plot(range(n+1), pi_list[:,0], color=\"red\", label=\"Hamburger\")\nax.plot(range(n+1), pi_list[:,1], color=\"green\", label=\"Pizza\")\nax.plot(range(n+1), pi_list[:,2], color=\"blue\", label=\"Hotdog\")\n\n\n\n\nTransition probability convergence plot.\n\n\n\n\nDefinition: (Doubly Stochastic Matrices)\nA matrix \\(P:=(p_{i,j})_{i,j\\in\\mathcal{S}}\\) is said to be doubly stochastic when both rows and columns sum to 1, that is \\[\n\\sum_{j\\in\\mathcal{X}}p_{i,j} = \\sum_{i\\in\\mathcal{X}}p_{i,j}=1.\n\\]\nTheorem: (Uniform Stationary Distributions for Doubly Stochastic MCs)\nA finite state homogenous MC with doubly stochastic transition probability matrix has a uniform stationary distribution.\nProof\nAssume \\(\\mathcal{X}\\) has \\(n\\) elements and define \\(\\pi = (\\pi_i)_{i\\in\\mathcal{X}}\\) where \\(\\pi_i=1/n\\). Then clearly\n\\[\n\\sum_{i\\in\\mathcal{X}} \\pi_i p_{i,j} = \\frac{1}{n}\\sum_{i\\in\\mathcal{X}}p_{i,j} = \\frac{1}{n} = \\pi_j,\n\\]\nholds for all \\(j\\in\\mathcal{X}\\). Since by definition \\(\\pi\\) is normalized we obtain the result.\n\\(\\square\\)"
  },
  {
    "objectID": "posts/welcome-post/index.html",
    "href": "posts/welcome-post/index.html",
    "title": "First Steps",
    "section": "",
    "text": "Hello and thank you for visiting my blog! In this blog I will post about my ongoing projects, new releases and anything else that I find interesting.\n\n\n\nImage taken by OC Gonzalez, find him on instagram at @OCVISUAL.\n\n\nGoing into my third year as a graduate student at UC Santa Barbara I wanted to make more of an effort to write and publish content that I felt might be helpful to anybody either looking to build their skills in mathematics, statistics and data science, or looking for advice on how to tackle both obtaining and commencing graduate education. This might be a good point to stress that, although I will take the utmost care and attention to ensure that anything I produce is accurate, I will almost surely make mistakes. If you spot any, please feel free to drop me a note here."
  },
  {
    "objectID": "posts/welcome-post/index.html#welcome",
    "href": "posts/welcome-post/index.html#welcome",
    "title": "First Steps",
    "section": "",
    "text": "Hello and thank you for visiting my blog! In this blog I will post about my ongoing projects, new releases and anything else that I find interesting.\n\n\n\nImage taken by OC Gonzalez, find him on instagram at @OCVISUAL.\n\n\nGoing into my third year as a graduate student at UC Santa Barbara I wanted to make more of an effort to write and publish content that I felt might be helpful to anybody either looking to build their skills in mathematics, statistics and data science, or looking for advice on how to tackle both obtaining and commencing graduate education. This might be a good point to stress that, although I will take the utmost care and attention to ensure that anything I produce is accurate, I will almost surely make mistakes. If you spot any, please feel free to drop me a note here."
  },
  {
    "objectID": "posts/welcome-post/index.html#plans-going-forward",
    "href": "posts/welcome-post/index.html#plans-going-forward",
    "title": "First Steps",
    "section": "üìã Plans going forward",
    "text": "üìã Plans going forward\nMy plans are to consistently produce useful and engaging content in the form of YouTube videos, website posts and projects, and study material. Some of the topics I am interested in writing about include:\n\nHelpful software for students:\n\nNotion for life planning and organization\nObsidian for study, document referencing and creation\nZotero for reference management\nTechnical writing using LaTeX and Markdown in Quarto\n\nTopics in mathematics and statistics:\n\nProbability Theory\nStochastic Processes\nLinear Algebra\nFinancial Mathematics\nStatistical Modelling\n\nTopics in data science:\n\nProgramming in R, Python, and SQL\nCoding projects\n\nCareer planning and guidance:\n\nHow to make career decisions\nHow to prepare for technical interviews\n\nAdvice for graduate students:\n\nHow to prepare class material using Quarto and GitHub\nHow to create a personal website / blog\n\n\nAdditionally I also plan to write about my ongoing research projects. I am currently involved in two topics, one investigating modelling groundwater in California and the other investigating the survival capacity of actively managed mutual funds in the United States."
  },
  {
    "objectID": "posts/welcome-post/index.html#teething-problems",
    "href": "posts/welcome-post/index.html#teething-problems",
    "title": "First Steps",
    "section": "ü¶∑ Teething Problems",
    "text": "ü¶∑ Teething Problems\nThis is my first time taking on such an ambitious plan of producing and publishing content. Although I apologize in advance for any broken links, poor formatting and missed posting deadlines, I hope that something I write might prove helpful!"
  }
]